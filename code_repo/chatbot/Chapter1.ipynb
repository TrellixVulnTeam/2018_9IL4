{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Chatbots in Python\n",
    "\n",
    "<strong>Course Description</strong>\n",
    "\n",
    "When done well, interacting with a computer through human language is incredibly powerful and also quite fun. Messaging and Voice-Controlled devices are the next big platforms, and conversational computing has a big role to play in creating engaging augmented and virtual reality experiences. This course will get you started on the path towards building such applications! There are a number of unique challenges to building these kinds of programs. The most obvious one is of course - how do I turn human language into machine instructions? In this course, you'll tackle this first with rule-based systems and then with machine learning. Some chat systems are designed to be useful, while others are just good fun. You will build one of each, and finally put everything together to make a helpful, friendly chatbot! And once you complete the course, you can learn how to <a src= \"https://www.datacamp.com/community/tutorials/facebook-chatbot-python-deploy\" >connect your chatbot to Facebook Messenger!</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1 : Chatbots 101\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this chapter, you'll learn how to build your first chatbot! After gaining a bit of historical context, you'll set up a basic structure for receiving text and responding to users, and then learn how to add the basic elements of personality. You'll then build rule-based systems for parsing text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T17:26:26.989928Z",
     "start_time": "2018-11-14T17:26:26.985135Z"
    }
   },
   "source": [
    "<video controls src=\"Intro_ConversationalAI.mp4\" width=\"560\" height=\"315\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:11.477221Z",
     "start_time": "2018-12-02T19:36:11.472801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : hello\n",
      "BOT : I can hear you! You said: hello\n"
     ]
    }
   ],
   "source": [
    "bot_template = \"BOT : {0}\"\n",
    "user_template = \"USER : {0}\"\n",
    "\n",
    "# Define a function that responds to a user's message: respond\n",
    "def respond(message):\n",
    "    # Concatenate the user's message to the end of a standard bot respone\n",
    "    bot_message = \"I can hear you! You said: \" + message\n",
    "    # Return the result\n",
    "    return bot_message\n",
    "\n",
    "# Define a function that sends a message to the bot: send_message\n",
    "def send_message(message):\n",
    "    # Print user_template including the user_message\n",
    "    print(user_template.format(message))\n",
    "    # Get the bot's response to the message\n",
    "    response = respond(message)\n",
    "    # Print the bot template including the bot's response.\n",
    "    print(bot_template.format(response))\n",
    "\n",
    "# Send a message to the bot\n",
    "send_message(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a personality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:11.692591Z",
     "start_time": "2018-12-02T19:36:11.688322Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define variables\n",
    "name = \"Greg\"\n",
    "weather = \"cloudy\"\n",
    "\n",
    "# Define a dictionary with the predefined responses\n",
    "responses = {\n",
    "  \"what's your name?\": \"my name is {0}\".format(name),\n",
    "  \"what's today's weather?\": \"the weather is {0}\".format(weather),\n",
    "  \"default\": \"default message\"\n",
    "}\n",
    "\n",
    "# Return the matching response if there is one, default otherwise\n",
    "def respond(message):\n",
    "    # Check if the message is in the responses\n",
    "    if message in responses:\n",
    "        # Return the matching message\n",
    "        bot_message = responses[message]\n",
    "    else:\n",
    "        # Return the \"default\" message\n",
    "        bot_message = responses[\"default\"]\n",
    "    return bot_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:11.698241Z",
     "start_time": "2018-12-02T19:36:11.694912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : what's your favorite color?\n",
      "BOT : default message\n"
     ]
    }
   ],
   "source": [
    "send_message(\"what's your favorite color?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:11.704560Z",
     "start_time": "2018-12-02T19:36:11.701189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : what's your name?\n",
      "BOT : my name is Greg\n"
     ]
    }
   ],
   "source": [
    "send_message(\"what's your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:11.709078Z",
     "start_time": "2018-12-02T19:36:11.706570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : what's today's weather?\n",
      "BOT : the weather is cloudy\n"
     ]
    }
   ],
   "source": [
    "send_message(\"what's today's weather?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding variety\n",
    "\n",
    "It can get a little boring hearing the same old answers over and over. In this exercise, you'll add some variation. If you ask your bot how it's feeling, it may equally well respond with \"oh I'm great!\" as with \"I'm very sad today\".\n",
    "\n",
    "Here, you'll use the random module - specifically random.choice(ls) - which randomly selects an element from a list ls.\n",
    "\n",
    "A dictionary called responses, which maps each message to a list of possible responses, has been defined for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:12.029246Z",
     "start_time": "2018-12-02T19:36:12.021661Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the random module\n",
    "import random\n",
    "\n",
    "name = \"Greg\"\n",
    "weather = \"cloudy\"\n",
    "\n",
    "# Define a dictionary containing a list of responses for each message\n",
    "responses = {\n",
    "  \"what's your name?\": [\n",
    "      \"my name is {0}\".format(name),\n",
    "      \"they call me {0}\".format(name),\n",
    "      \"I go by {0}\".format(name)\n",
    "   ],\n",
    "  \"what's today's weather?\": [\n",
    "      \"the weather is {0}\".format(weather),\n",
    "      \"it's {0} today\".format(weather)\n",
    "    ],\n",
    "  \"default\": [\"default message\"]\n",
    "}\n",
    "\n",
    "# Use random.choice() to choose a matching response\n",
    "def respond(message):\n",
    "    # Check if the message is in the responses\n",
    "    if message in responses:\n",
    "        # Return a random matching response\n",
    "        bot_message = random.choice(responses[message])\n",
    "    else:\n",
    "        # Return a random \"default\" response\n",
    "        bot_message = random.choice(responses[\"default\"])\n",
    "    return bot_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:12.035994Z",
     "start_time": "2018-12-02T19:36:12.032501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : what's your name?\n",
      "BOT : my name is Greg\n"
     ]
    }
   ],
   "source": [
    "send_message(\"what's your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:12.041993Z",
     "start_time": "2018-12-02T19:36:12.038629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : what's your name?\n",
      "BOT : I go by Greg\n"
     ]
    }
   ],
   "source": [
    "send_message(\"what's your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:12.047449Z",
     "start_time": "2018-12-02T19:36:12.044553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : what's your name?\n",
      "BOT : they call me Greg\n"
     ]
    }
   ],
   "source": [
    "send_message(\"what's your name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ELIZA I: asking questions\n",
    "Asking questions is a great way to create an engaging conversation. Here, you'll create the very first hint of ELIZA's famous personality, by responding to statements with a question and responding to questions with answers.\n",
    "\n",
    "A dictionary of responses with \"question\" and \"statement\" as keys, and lists of appropriate responses as values has already been defined for you. Explore this in the Shell with responses.keys() and responses[\"question\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:12.387647Z",
     "start_time": "2018-12-02T19:36:12.383617Z"
    }
   },
   "outputs": [],
   "source": [
    "responses = {'question': [\"I don't know :(\", 'you tell me!'],\n",
    " 'statement': ['tell me more!',\n",
    "  'why do you think that?',\n",
    "  'how long have you felt this way?',\n",
    "  'I find that extremely interesting',\n",
    "  'can you back that up?',\n",
    "  'oh wow!',\n",
    "  ':)']}\n",
    "\n",
    "def respond(message):\n",
    "    # Check for a question mark\n",
    "    if message.endswith(\"?\"):\n",
    "        # Return a random question\n",
    "        return random.choice(responses[\"question\"])\n",
    "    # Return a random statement\n",
    "    return random.choice(responses[\"statement\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:12.395913Z",
     "start_time": "2018-12-02T19:36:12.391978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : what's today's weather?\n",
      "BOT : I don't know :(\n",
      "USER : what's today's weather?\n",
      "BOT : I don't know :(\n"
     ]
    }
   ],
   "source": [
    "# Send messages ending in a question mark\n",
    "send_message(\"what's today's weather?\")\n",
    "send_message(\"what's today's weather?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:12.401229Z",
     "start_time": "2018-12-02T19:36:12.397989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : I love building chatbots\n",
      "BOT : can you back that up?\n",
      "USER : I love building chatbots\n",
      "BOT : I find that extremely interesting\n"
     ]
    }
   ],
   "source": [
    "# Send messages which don't end with a question mark\n",
    "send_message(\"I love building chatbots\")\n",
    "send_message(\"I love building chatbots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Munging with regular expressions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ELIZA II: Extracting key phrases\n",
    "The really clever thing about ELIZA is the way the program appears to understand what you told it, by occasionally including phrases uttered by the user in its responses.\n",
    "\n",
    "In this exercise, you will match messages against some common patterns and extract phrases using re.search(). A dictionary called rules has already been defined, which matches the following patterns:\n",
    "\n",
    "\"do you think (.*)\"\n",
    "\"do you remember (.*)\"\n",
    "\"I want (.*)\"\n",
    "\"if (.*)\"\n",
    "Inspect this dictionary in the Shell before starting the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:12.862153Z",
     "start_time": "2018-12-02T19:36:12.855263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Yes .. and?', None)\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "rules = {'I want (.*)': ['What would it mean if you got {0}',\n",
    "  'Why do you want {0}',\n",
    "  \"What's stopping you from getting {0}\"],\n",
    " 'do you remember (.*)': ['Did you think I would forget {0}',\n",
    "  \"Why haven't you been able to forget {0}\",\n",
    "  'What about {0}',\n",
    "  'Yes .. and?'],\n",
    " 'do you think (.*)': ['if {0}? Absolutely.', 'No chance'],\n",
    " 'if (.*)': [\"Do you really think it's likely that {0}\",\n",
    "  'Do you wish that {0}',\n",
    "  'What do you think about {0}',\n",
    "  'Really--if {0}']}\n",
    "\n",
    "# Define match_rule()\n",
    "def match_rule(rules, message):\n",
    "    response, phrase = \"default\", None\n",
    "    \n",
    "    # Iterate over the rules dictionary\n",
    "    for pattern, responses in rules.items():\n",
    "        # Create a match object\n",
    "        match = re.search(pattern,message)\n",
    "        if match is not None:\n",
    "            # Choose a random response\n",
    "            response = random.choice(responses)\n",
    "            if '{0}' in response:\n",
    "                phrase = match.group(1)\n",
    "    # Return the response and phrase\n",
    "    return response, phrase\n",
    "\n",
    "# Test match_rule\n",
    "print(match_rule(rules, \"do you remember your last birthday\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:12.867208Z",
     "start_time": "2018-12-02T19:36:12.864337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Yes .. and?', None)\n"
     ]
    }
   ],
   "source": [
    "print(match_rule(rules, \"do you remember your last birthday\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ELIZA III: Pronouns\n",
    "To make responses grammatically coherent, you'll want to transform the extracted phrases from first to second person and vice versa. In English, conjugating verbs is easy, and simply swapping \"I\" and 'you', \"my\" and \"your\" works in most cases.\n",
    "\n",
    "In this exercise, you'll define a function called replace_pronouns() which uses re.sub() to map \"me\" and \"my\" to \"you\" and \"your\" (and vice versa) in a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:13.151873Z",
     "start_time": "2018-12-02T19:36:13.146322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your last birthday\n",
      "when me went to florida\n",
      "i had your own castle\n"
     ]
    }
   ],
   "source": [
    "# Define replace_pronouns()\n",
    "def replace_pronouns(message):\n",
    "\n",
    "    message = message.lower()\n",
    "    if 'me' in message:\n",
    "        # Replace 'me' with 'you'\n",
    "        return re.sub('me','you',message)\n",
    "    if 'my' in message:\n",
    "        # Replace 'my' with 'your'\n",
    "        return re.sub('my','your',message)\n",
    "    if 'your' in message:\n",
    "        # Replace 'your' with 'my'\n",
    "        return re.sub('your','my',message)\n",
    "    if 'you' in message:\n",
    "        # Replace 'you' with 'me'\n",
    "        return re.sub('you','me',message)\n",
    "\n",
    "    return message\n",
    "\n",
    "print(replace_pronouns(\"my last birthday\"))\n",
    "print(replace_pronouns(\"when you went to Florida\"))\n",
    "print(replace_pronouns(\"I had my own castle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ELIZA IV: Putting it all together\n",
    "Now you're going to put it all together and experience the magic! The match_rule(), send_message(), and replace_pronouns() functions have already been defined, and the rules dictionary is available in your workspace.\n",
    "\n",
    "Your job here is to write a function called respond() with a single argument message which creates an appropriate response to be handled by send_message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:13.438313Z",
     "start_time": "2018-12-02T19:36:13.432394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : do you remember your last birthday\n",
      "BOT : What about my last birthday\n",
      "USER : do you think humans should be worried about AI\n",
      "BOT : No chance\n",
      "USER : I want a robot friend\n",
      "BOT : Why do you want a robot friend\n",
      "USER : what if you could be anything you wanted\n",
      "BOT : Do you wish that me could be anything me wanted\n"
     ]
    }
   ],
   "source": [
    "# Define respond()\n",
    "def respond(message):\n",
    "    # Call match_rule\n",
    "    response, phrase = match_rule(rules,message)\n",
    "    if '{0}' in response:\n",
    "        # Replace the pronouns in the phrase\n",
    "        phrase = replace_pronouns(phrase)\n",
    "        # Include the phrase in the response\n",
    "        response = response.format(phrase)\n",
    "    return response\n",
    "\n",
    "# Send the messages\n",
    "send_message(\"do you remember your last birthday\")\n",
    "send_message(\"do you think humans should be worried about AI\")\n",
    "send_message(\"I want a robot friend\")\n",
    "send_message(\"what if you could be anything you wanted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2 : Understanding natural language\n",
    "\n",
    "Here, you'll use machine learning to turn natural language into structured data using spaCy, scikit-learn, and rasa NLU. You'll start with a refresher on the theoretical foundations, and then move on to building models using the ATIS dataset, which contains thousands of sentences from real people interacting with a flight booking system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T17:05:47.077179Z",
     "start_time": "2018-11-14T17:05:47.074241Z"
    }
   },
   "source": [
    "### Understanding Intents and Entities :NLU (Natural Language Understanding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intent classification with regex I\n",
    "You'll begin by implementing a very simple way to recognise intents - just looking for the presence of keywords.\n",
    "\n",
    "A dictionary keywords has already been defined. It has the intents \"greet\", \"goodbye\", and \"thankyou\" as keys, and lists of keywords as the corresponding values. For example, keywords[\"greet\"] is set to \"[\"hello\",\"hi\",\"hey\"].\n",
    "\n",
    "Also defined is a second dictionary, responses, indicating how the bot should respond to each of these intents. It also has a default response with the key \"default\".\n",
    "\n",
    "The function send_message(), along with the bot and user templates have also already been defined. Your job in this exercise is to create a dictionary with the intents as keys and regex objects as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:14.250642Z",
     "start_time": "2018-12-02T19:36:14.245414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'greet': 'r\"(hello|hi|hey)\"', 'goodbye': 'r\"(bye|farewell)\"', 'thankyou': 'r\"(thank|thx)\"'}\n"
     ]
    }
   ],
   "source": [
    "keywords = {'greet': ['hello', 'hi', 'hey'], 'goodbye': ['bye', 'farewell'], 'thankyou': ['thank', 'thx']}\n",
    "responses = {'default': 'default message',\n",
    " 'goodbye': 'goodbye for now',\n",
    " 'greet': 'Hello you! :)',\n",
    " 'thankyou': 'you are very welcome'}\n",
    "# Define a dictionary of patterns\n",
    "patterns = {}\n",
    "\n",
    "# Iterate over the keywords dictionary\n",
    "for intent, keys in keywords.items():\n",
    "    # Create regular expressions and compile them into pattern objects\n",
    "    patterns[intent] = 'r\"('+'|'.join(keys)+')\"'\n",
    "    \n",
    "# Print the patterns\n",
    "print(patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intent classification with regex II\n",
    "With your patterns dictionary created, it's now time to define a function to find the intent of a message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:14.449060Z",
     "start_time": "2018-12-02T19:36:14.442620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : hello!\n",
      "BOT : default message\n",
      "USER : bye byeee\n",
      "BOT : default message\n",
      "USER : thanks very much!\n",
      "BOT : default message\n"
     ]
    }
   ],
   "source": [
    "# Define a function to find the intent of a message\n",
    "def match_intent(message):\n",
    "    matched_intent = None\n",
    "    for intent, pattern in patterns.items():\n",
    "        # Check if the pattern occurs in the message \n",
    "        if re.search(pattern,message):\n",
    "            matched_intent = intent\n",
    "    return matched_intent\n",
    "\n",
    "# Define a respond function\n",
    "def respond(message):\n",
    "    # Call the match_intent function\n",
    "    intent = match_intent(message)\n",
    "    # Fall back to the default response\n",
    "    key = \"default\"\n",
    "    if intent in responses:\n",
    "        key = intent\n",
    "    return responses[key]\n",
    "\n",
    "# Send messages\n",
    "send_message(\"hello!\")\n",
    "send_message(\"bye byeee\")\n",
    "send_message(\"thanks very much!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entity extraction with regex\n",
    "Now you'll use another simple method, this time for finding a person's name in a sentence such as \"hello, my name is David Copperfield\".\n",
    "\n",
    "You'll look for the keywords \"name\" or \"call(ed)\", and find capitalized words using regex and assume those are names. Your job in this exercise is to define a find_name() function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:14.705008Z",
     "start_time": "2018-12-02T19:36:14.698609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : my name is David Copperfield\n",
      "BOT : Hello, David Copperfield!\n",
      "USER : call me Ishmael\n",
      "BOT : Hello, Ishmael!\n",
      "USER : People call me Cassandra\n",
      "BOT : Hello, People Cassandra!\n"
     ]
    }
   ],
   "source": [
    "# Define find_name()\n",
    "def find_name(message):\n",
    "    name = None\n",
    "    # Create a pattern for checking if the keywords occur\n",
    "    name_keyword = re.compile('[name|call]*')\n",
    "    # Create a pattern for finding capitalized words\n",
    "    name_pattern = re.compile('[A-Z]{1}[a-z]*')\n",
    "    if name_keyword.search(message):\n",
    "        # Get the matching words in the string\n",
    "        name_words = name_pattern.findall(message)\n",
    "        if len(name_words) > 0:\n",
    "            # Return the name if the keywords are present\n",
    "            name = ' '.join(name_words)\n",
    "    return name\n",
    "\n",
    "# Define respond()\n",
    "def respond(message):\n",
    "    # Find the name\n",
    "    name = find_name(message)\n",
    "    if name is None:\n",
    "        return \"Hi there!\"\n",
    "    else:\n",
    "        return \"Hello, {0}!\".format(name)\n",
    "\n",
    "# Send messages\n",
    "send_message(\"my name is David Copperfield\")\n",
    "send_message(\"call me Ishmael\")\n",
    "send_message(\"People call me Cassandra\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors\n",
    "\n",
    "#### word vectors with spaCy\n",
    "In this exercise you'll get your first experience with word vectors! You're going to use the ATIS dataset, which contains thousands of sentences from real people interacting with a flight booking system.\n",
    "\n",
    "The user utterances are available in the list sentences, and the corresponding intents in labels.\n",
    "\n",
    "Your job is to create a 2D array X with as many rows as there are sentences in the dataset, where each row is a vector describing that sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:14.934004Z",
     "start_time": "2018-12-02T19:36:14.928532Z"
    }
   },
   "outputs": [],
   "source": [
    "##ATIS Dataset\n",
    "sentences = [' i want to fly from boston at 838 am and arrive in denver at 1110 in the morning',\n",
    " ' what flights are available from pittsburgh to baltimore on thursday morning',\n",
    " ' what is the arrival time in san francisco for the 755 am flight leaving washington',\n",
    " ' cheapest airfare from tacoma to orlando',\n",
    " ' round trip fares from pittsburgh to philadelphia under 1000 dollars',\n",
    " ' i need a flight tomorrow from columbus to minneapolis',\n",
    " ' what kind of aircraft is used on a flight from cleveland to dallas',\n",
    " ' show me the flights from pittsburgh to los angeles on thursday',\n",
    " ' all flights from boston to washington',\n",
    " ' what kind of ground transportation is available in denver',\n",
    " ' show me the flights from dallas to san francisco',\n",
    " ' show me the flights from san diego to newark by way of houston',\n",
    " ' what is the cheapest flight from boston to bwi',\n",
    " ' all flights to baltimore after 6 pm',\n",
    " ' show me the first class fares from boston to denver',\n",
    " ' show me the ground transportation in denver',\n",
    " ' all flights from denver to pittsburgh leaving after 6 pm and before 7 pm',\n",
    " ' i need information on flights for tuesday leaving baltimore for dallas dallas to boston and boston to baltimore',\n",
    " ' please give me the flights from boston to pittsburgh on thursday of next week',\n",
    " ' i would like to fly from denver to pittsburgh on united airlines',\n",
    " ' show me the flights from san diego to newark',\n",
    " ' please list all first class flights on united from denver to baltimore',\n",
    " ' what kinds of planes are used by american airlines',\n",
    " \" i'd like to have some information on a ticket from denver to pittsburgh and atlanta\",\n",
    " \" i'd like to book a flight from atlanta to denver\",\n",
    " ' which airline serves denver pittsburgh and atlanta',\n",
    " \" show me all flights from boston to pittsburgh on wednesday of next week which leave boston after 2 o'clock pm\",\n",
    " ' atlanta ground transportation',\n",
    " ' i also need service from dallas to boston arriving by noon',\n",
    " ' show me the cheapest round trip fare from baltimore to dallas']\n",
    "\n",
    "labels = ['atis_flight',\n",
    " 'atis_flight',\n",
    " 'atis_flight_time',\n",
    " 'atis_airfare',\n",
    " 'atis_airfare',\n",
    " 'atis_flight',\n",
    " 'atis_aircraft',\n",
    " 'atis_flight',\n",
    " 'atis_flight',\n",
    " 'atis_ground_service',\n",
    " 'atis_flight',\n",
    " 'atis_flight',\n",
    " 'atis_flight',\n",
    " 'atis_flight',\n",
    " 'atis_airfare',\n",
    " 'atis_ground_service',\n",
    " 'atis_flight',\n",
    " 'atis_flight',\n",
    " 'atis_flight',\n",
    " 'atis_flight',\n",
    " 'atis_flight',\n",
    " 'atis_flight',\n",
    " 'atis_aircraft',\n",
    " 'atis_airfare',\n",
    " 'atis_flight',\n",
    " 'atis_airline',\n",
    " 'atis_flight',\n",
    " 'atis_ground_service',\n",
    " 'atis_flight',\n",
    " 'atis_airfare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:28.162161Z",
     "start_time": "2018-12-02T19:36:14.936239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "# Load the spacy model: nlp\n",
    "nlp = spacy.load('en_md')\n",
    "\n",
    "# Calculate the length of sentences\n",
    "n_sentences = len(sentences)\n",
    "\n",
    "# Calculate the dimensionality of nlp\n",
    "embedding_dim = nlp.vocab.vectors_length\n",
    "print(embedding_dim)\n",
    "\n",
    "# Initialize the array with zeros: X\n",
    "X = np.zeros((n_sentences, embedding_dim))\n",
    "X[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:28.426983Z",
     "start_time": "2018-12-02T19:36:28.163959Z"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate over the sentences\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    # Pass each each sentence to the nlp object to create a document\n",
    "    doc = nlp(sentence)\n",
    "    # Save the document's .vector attribute to the corresponding row in X\n",
    "    X[idx, :] = doc.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:28.445614Z",
     "start_time": "2018-12-02T19:36:28.429448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr1/anaconda3/envs/jupyter_env/lib/python3.7/site-packages\n"
     ]
    }
   ],
   "source": [
    "# Trouble shooting\n",
    "from distutils.sysconfig import get_python_lib\n",
    "print(get_python_lib())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intent classification with sklearn\n",
    "An array X containing vectors describing each of the sentences in the ATIS dataset has been created for you, along with a 1D array y containing the labels. The labels are integers corresponding to the intents in the dataset. For example, label 0 corresponds to the intent atis_flight.\n",
    "\n",
    "Now, you'll use the scikit-learn library to train a classifier on this same dataset. Specifically, you will fit and evaluate a support vector classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:28.632530Z",
     "start_time": "2018-12-02T19:36:28.448158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['atis.zip', 'atis.dict.intent.csv', 'atis.dict.slots.csv', 'atis.dict.vocab.csv', 'atis.test.intent.csv', 'atis.test.pkl', 'atis.test.query.csv', 'atis.test.slots.csv', 'atis.train.intent.csv', 'atis.train.pkl', 'atis.train.query.csv', 'atis.train.slots.csv']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "print(os.listdir(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:28.639745Z",
     "start_time": "2018-12-02T19:36:28.634559Z"
    }
   },
   "outputs": [],
   "source": [
    "#load ATIS dataset from Kaggle - ATIS = Airline Travel Information System\n",
    "#https://www.kaggle.com/siddhadev/atis-dataset/notebook\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "DATA_DIR=\"data\"\n",
    "\n",
    "def load_ds(fname='atis.train.pkl'):\n",
    "    with open(fname, 'rb') as stream:\n",
    "        ds,dicts = pickle.load(stream)\n",
    "    print(ds.keys(),dicts.keys())\n",
    "    print('Done  loading: ', fname)\n",
    "    print('      samples: {:4d}'.format(len(ds['query'])))\n",
    "    print('   vocab_size: {:4d}'.format(len(dicts['token_ids'])))\n",
    "    print('   slot count: {:4d}'.format(len(dicts['slot_ids'])))\n",
    "    print(' intent count: {:4d}'.format(len(dicts['intent_ids'])))\n",
    "    return ds,dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:36:28.651314Z",
     "start_time": "2018-12-02T19:36:28.641704Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show some examples from dataset\n",
    "def extract_sentences(ds,dicts):\n",
    "    train_sentences = []\n",
    "    df = pd.DataFrame.from_dict(ds)\n",
    "    \n",
    "    t2i, s2i, in2i = map(dicts.get, ['token_ids', 'slot_ids','intent_ids'])\n",
    "    i2t, i2s, i2in = map(lambda d: {d[k]:k for k in d.keys()}, [t2i,s2i,in2i])\n",
    "    #i2tv, i2sv, i2inv = map(lambda d: {d[k]:k for k in d.values()}, [t2i,s2i,in2i])\n",
    "    query, slots, intent =  map(ds.get, ['query', 'slot_labels', 'intent_labels'])\n",
    "\n",
    "    #for i in range(5):\n",
    "    #for i in range(len(df)):\n",
    "    big_add = {}\n",
    "    \n",
    "    for i,row in df.iterrows():\n",
    "        curr = {}\n",
    "        \n",
    "        curr['intent_class'] = i2in[intent[i][0]]\n",
    "        curr['intent_val'] = intent[i][0]\n",
    "        curr['question'] = ' '.join(map(i2t.get, query[i]))\n",
    "        curr['slot_ids'] = [ i2s[slots[i][j]] for j in range(len(query[i]))]\n",
    "        curr['slot_label_list'] = [ i2t[query[i][j]] for j in range(len(query[i]))]\n",
    "        curr['slots'] = [ (i2t[query[i][j]],i2s[slots[i][j]]) for j in range(len(query[i]))]\n",
    "        \n",
    "        big_add[i] = curr\n",
    "        \n",
    "        doc = nlp(sentence)\n",
    "        curr['vector'] = doc.vector\n",
    "        #print(len(doc.vector))\n",
    "        '''\n",
    "        sentence = '{:4d}:{:>15}: {}'.format(i, i2in[intent[i][0]],' '.join(map(i2t.get, query[i])))\n",
    "        for j in range(len(query[i])):\n",
    "            print('{:>33} {:>40}'.format(i2t[query[i][j]],i2s[slots[i][j]]  ))'''\n",
    "        #print('*'*74)\n",
    "    add_df = pd.DataFrame.from_dict(big_add,orient='index')\n",
    "    result = pd.concat([df, add_df], axis=1, sort=False)\n",
    "    print(result.head())\n",
    "    \n",
    "    #X_shape = (len(result),nlp.vocab.vectors_length)\n",
    "    #X = np.zeros(X_train_shape)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:37:18.704970Z",
     "start_time": "2018-12-02T19:36:28.653124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['slot_labels', 'query', 'intent_labels']) dict_keys(['token_ids', 'slot_ids', 'intent_ids'])\n",
      "Done  loading:  data/atis.train.pkl\n",
      "      samples: 4978\n",
      "   vocab_size:  943\n",
      "   slot count:  129\n",
      " intent count:   26\n",
      "                                         slot_labels  \\\n",
      "0  [128, 128, 128, 128, 128, 128, 48, 128, 35, 10...   \n",
      "1  [128, 128, 128, 128, 128, 128, 48, 128, 78, 12...   \n",
      "2  [128, 128, 128, 128, 45, 108, 128, 48, 110, 12...   \n",
      "3              [128, 21, 128, 128, 48, 128, 78, 128]   \n",
      "4  [128, 66, 119, 128, 128, 48, 128, 78, 21, 38, ...   \n",
      "\n",
      "                                               query intent_labels  \\\n",
      "0  [178, 479, 902, 851, 431, 444, 266, 240, 168, ...          [14]   \n",
      "1  [178, 916, 429, 228, 244, 444, 682, 851, 247, ...          [14]   \n",
      "2  [178, 916, 498, 827, 234, 849, 482, 739, 440, ...          [19]   \n",
      "3           [178, 296, 197, 444, 810, 851, 667, 179]           [3]   \n",
      "4  [178, 730, 870, 415, 444, 682, 851, 678, 886, ...           [3]   \n",
      "\n",
      "  intent_class  intent_val                                           question  \\\n",
      "0       flight          14  BOS i want to fly from boston at 838 am and ar...   \n",
      "1       flight          14  BOS what flights are available from pittsburgh...   \n",
      "2  flight_time          19  BOS what is the arrival time in san francisco ...   \n",
      "3      airfare           3    BOS cheapest airfare from tacoma to orlando EOS   \n",
      "4      airfare           3  BOS round trip fares from pittsburgh to philad...   \n",
      "\n",
      "                                            slot_ids  \\\n",
      "0  [O, O, O, O, O, O, B-fromloc.city_name, O, B-d...   \n",
      "1  [O, O, O, O, O, O, B-fromloc.city_name, O, B-t...   \n",
      "2  [O, O, O, O, B-flight_time, I-flight_time, O, ...   \n",
      "3  [O, B-cost_relative, O, O, B-fromloc.city_name...   \n",
      "4  [O, B-round_trip, I-round_trip, O, O, B-fromlo...   \n",
      "\n",
      "                                     slot_label_list  \\\n",
      "0  [BOS, i, want, to, fly, from, boston, at, 838,...   \n",
      "1  [BOS, what, flights, are, available, from, pit...   \n",
      "2  [BOS, what, is, the, arrival, time, in, san, f...   \n",
      "3  [BOS, cheapest, airfare, from, tacoma, to, orl...   \n",
      "4  [BOS, round, trip, fares, from, pittsburgh, to...   \n",
      "\n",
      "                                               slots  \\\n",
      "0  [(BOS, O), (i, O), (want, O), (to, O), (fly, O...   \n",
      "1  [(BOS, O), (what, O), (flights, O), (are, O), ...   \n",
      "2  [(BOS, O), (what, O), (is, O), (the, O), (arri...   \n",
      "3  [(BOS, O), (cheapest, B-cost_relative), (airfa...   \n",
      "4  [(BOS, O), (round, B-round_trip), (trip, I-rou...   \n",
      "\n",
      "                                              vector  \n",
      "0  [-0.0010358343, 0.017182918, -0.041435994, 0.0...  \n",
      "1  [-0.0010358343, 0.017182918, -0.041435994, 0.0...  \n",
      "2  [-0.0010358343, 0.017182918, -0.041435994, 0.0...  \n",
      "3  [-0.0010358343, 0.017182918, -0.041435994, 0.0...  \n",
      "4  [-0.0010358343, 0.017182918, -0.041435994, 0.0...  \n",
      "dict_keys(['slot_labels', 'query', 'intent_labels']) dict_keys(['token_ids', 'slot_ids', 'intent_ids'])\n",
      "Done  loading:  data/atis.test.pkl\n",
      "      samples:  893\n",
      "   vocab_size:  943\n",
      "   slot count:  129\n",
      " intent count:   26\n",
      "                                         slot_labels  \\\n",
      "0  [128, 128, 128, 128, 128, 128, 128, 128, 128, ...   \n",
      "1  [128, 128, 28, 27, 128, 128, 128, 128, 128, 48...   \n",
      "2  [128, 128, 28, 27, 128, 128, 128, 128, 128, 12...   \n",
      "3  [128, 128, 128, 128, 128, 128, 128, 66, 119, 1...   \n",
      "4  [128, 128, 128, 128, 128, 128, 128, 48, 128, 7...   \n",
      "\n",
      "                                               query intent_labels  \\\n",
      "0  [178, 479, 932, 545, 851, 423, 180, 428, 444, ...          [14]   \n",
      "1  [178, 654, 227, 425, 479, 617, 180, 847, 444, ...           [3]   \n",
      "2  [178, 654, 227, 425, 479, 617, 180, 428, 452, ...          [14]   \n",
      "3  [178, 479, 932, 545, 180, 428, 868, 656, 906, ...          [14]   \n",
      "4  [178, 479, 932, 545, 180, 428, 444, 667, 851, ...          [14]   \n",
      "\n",
      "  intent_class  intent_val                                           question  \\\n",
      "0       flight          14  BOS i would like to find a flight from charlot...   \n",
      "1      airfare           3  BOS on april first i need a ticket from tacoma...   \n",
      "2       flight          14  BOS on april first i need a flight going from ...   \n",
      "3       flight          14  BOS i would like a flight traveling one way fr...   \n",
      "4       flight          14  BOS i would like a flight from orlando to salt...   \n",
      "\n",
      "                                            slot_ids  \\\n",
      "0  [O, O, O, O, O, O, O, O, O, B-fromloc.city_nam...   \n",
      "1  [O, O, B-depart_date.month_name, B-depart_date...   \n",
      "2  [O, O, B-depart_date.month_name, B-depart_date...   \n",
      "3  [O, O, O, O, O, O, O, B-round_trip, I-round_tr...   \n",
      "4  [O, O, O, O, O, O, O, B-fromloc.city_name, O, ...   \n",
      "\n",
      "                                     slot_label_list  \\\n",
      "0  [BOS, i, would, like, to, find, a, flight, fro...   \n",
      "1  [BOS, on, april, first, i, need, a, ticket, fr...   \n",
      "2  [BOS, on, april, first, i, need, a, flight, go...   \n",
      "3  [BOS, i, would, like, a, flight, traveling, on...   \n",
      "4  [BOS, i, would, like, a, flight, from, orlando...   \n",
      "\n",
      "                                               slots  \\\n",
      "0  [(BOS, O), (i, O), (would, O), (like, O), (to,...   \n",
      "1  [(BOS, O), (on, O), (april, B-depart_date.mont...   \n",
      "2  [(BOS, O), (on, O), (april, B-depart_date.mont...   \n",
      "3  [(BOS, O), (i, O), (would, O), (like, O), (a, ...   \n",
      "4  [(BOS, O), (i, O), (would, O), (like, O), (a, ...   \n",
      "\n",
      "                                              vector  \n",
      "0  [-0.0010358343, 0.017182918, -0.041435994, 0.0...  \n",
      "1  [-0.0010358343, 0.017182918, -0.041435994, 0.0...  \n",
      "2  [-0.0010358343, 0.017182918, -0.041435994, 0.0...  \n",
      "3  [-0.0010358343, 0.017182918, -0.041435994, 0.0...  \n",
      "4  [-0.0010358343, 0.017182918, -0.041435994, 0.0...  \n"
     ]
    }
   ],
   "source": [
    "train_ds, tr_dicts = load_ds(os.path.join(DATA_DIR,'atis.train.pkl'))\n",
    "train_df = extract_sentences(train_ds, tr_dicts)\n",
    "test_ds, te_dicts  = load_ds(os.path.join(DATA_DIR,'atis.test.pkl'))\n",
    "test_df = extract_sentences(test_ds, te_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:37:18.744079Z",
     "start_time": "2018-12-02T19:37:18.706805Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slot_labels</th>\n",
       "      <th>query</th>\n",
       "      <th>intent_labels</th>\n",
       "      <th>intent_class</th>\n",
       "      <th>intent_val</th>\n",
       "      <th>question</th>\n",
       "      <th>slot_ids</th>\n",
       "      <th>slot_label_list</th>\n",
       "      <th>slots</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 128, 128, 128, 128, 128, 48, 128, 35, 10...</td>\n",
       "      <td>[178, 479, 902, 851, 431, 444, 266, 240, 168, ...</td>\n",
       "      <td>[14]</td>\n",
       "      <td>flight</td>\n",
       "      <td>14</td>\n",
       "      <td>BOS i want to fly from boston at 838 am and ar...</td>\n",
       "      <td>[O, O, O, O, O, O, B-fromloc.city_name, O, B-d...</td>\n",
       "      <td>[BOS, i, want, to, fly, from, boston, at, 838,...</td>\n",
       "      <td>[(BOS, O), (i, O), (want, O), (to, O), (fly, O...</td>\n",
       "      <td>[-0.0010358343, 0.017182918, -0.041435994, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[128, 128, 128, 128, 128, 128, 48, 128, 78, 12...</td>\n",
       "      <td>[178, 916, 429, 228, 244, 444, 682, 851, 247, ...</td>\n",
       "      <td>[14]</td>\n",
       "      <td>flight</td>\n",
       "      <td>14</td>\n",
       "      <td>BOS what flights are available from pittsburgh...</td>\n",
       "      <td>[O, O, O, O, O, O, B-fromloc.city_name, O, B-t...</td>\n",
       "      <td>[BOS, what, flights, are, available, from, pit...</td>\n",
       "      <td>[(BOS, O), (what, O), (flights, O), (are, O), ...</td>\n",
       "      <td>[-0.0010358343, 0.017182918, -0.041435994, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[128, 128, 128, 128, 45, 108, 128, 48, 110, 12...</td>\n",
       "      <td>[178, 916, 498, 827, 234, 849, 482, 739, 440, ...</td>\n",
       "      <td>[19]</td>\n",
       "      <td>flight_time</td>\n",
       "      <td>19</td>\n",
       "      <td>BOS what is the arrival time in san francisco ...</td>\n",
       "      <td>[O, O, O, O, B-flight_time, I-flight_time, O, ...</td>\n",
       "      <td>[BOS, what, is, the, arrival, time, in, san, f...</td>\n",
       "      <td>[(BOS, O), (what, O), (is, O), (the, O), (arri...</td>\n",
       "      <td>[-0.0010358343, 0.017182918, -0.041435994, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[128, 21, 128, 128, 48, 128, 78, 128]</td>\n",
       "      <td>[178, 296, 197, 444, 810, 851, 667, 179]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>airfare</td>\n",
       "      <td>3</td>\n",
       "      <td>BOS cheapest airfare from tacoma to orlando EOS</td>\n",
       "      <td>[O, B-cost_relative, O, O, B-fromloc.city_name...</td>\n",
       "      <td>[BOS, cheapest, airfare, from, tacoma, to, orl...</td>\n",
       "      <td>[(BOS, O), (cheapest, B-cost_relative), (airfa...</td>\n",
       "      <td>[-0.0010358343, 0.017182918, -0.041435994, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[128, 66, 119, 128, 128, 48, 128, 78, 21, 38, ...</td>\n",
       "      <td>[178, 730, 870, 415, 444, 682, 851, 678, 886, ...</td>\n",
       "      <td>[3]</td>\n",
       "      <td>airfare</td>\n",
       "      <td>3</td>\n",
       "      <td>BOS round trip fares from pittsburgh to philad...</td>\n",
       "      <td>[O, B-round_trip, I-round_trip, O, O, B-fromlo...</td>\n",
       "      <td>[BOS, round, trip, fares, from, pittsburgh, to...</td>\n",
       "      <td>[(BOS, O), (round, B-round_trip), (trip, I-rou...</td>\n",
       "      <td>[-0.0010358343, 0.017182918, -0.041435994, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         slot_labels  \\\n",
       "0  [128, 128, 128, 128, 128, 128, 48, 128, 35, 10...   \n",
       "1  [128, 128, 128, 128, 128, 128, 48, 128, 78, 12...   \n",
       "2  [128, 128, 128, 128, 45, 108, 128, 48, 110, 12...   \n",
       "3              [128, 21, 128, 128, 48, 128, 78, 128]   \n",
       "4  [128, 66, 119, 128, 128, 48, 128, 78, 21, 38, ...   \n",
       "\n",
       "                                               query intent_labels  \\\n",
       "0  [178, 479, 902, 851, 431, 444, 266, 240, 168, ...          [14]   \n",
       "1  [178, 916, 429, 228, 244, 444, 682, 851, 247, ...          [14]   \n",
       "2  [178, 916, 498, 827, 234, 849, 482, 739, 440, ...          [19]   \n",
       "3           [178, 296, 197, 444, 810, 851, 667, 179]           [3]   \n",
       "4  [178, 730, 870, 415, 444, 682, 851, 678, 886, ...           [3]   \n",
       "\n",
       "  intent_class  intent_val                                           question  \\\n",
       "0       flight          14  BOS i want to fly from boston at 838 am and ar...   \n",
       "1       flight          14  BOS what flights are available from pittsburgh...   \n",
       "2  flight_time          19  BOS what is the arrival time in san francisco ...   \n",
       "3      airfare           3    BOS cheapest airfare from tacoma to orlando EOS   \n",
       "4      airfare           3  BOS round trip fares from pittsburgh to philad...   \n",
       "\n",
       "                                            slot_ids  \\\n",
       "0  [O, O, O, O, O, O, B-fromloc.city_name, O, B-d...   \n",
       "1  [O, O, O, O, O, O, B-fromloc.city_name, O, B-t...   \n",
       "2  [O, O, O, O, B-flight_time, I-flight_time, O, ...   \n",
       "3  [O, B-cost_relative, O, O, B-fromloc.city_name...   \n",
       "4  [O, B-round_trip, I-round_trip, O, O, B-fromlo...   \n",
       "\n",
       "                                     slot_label_list  \\\n",
       "0  [BOS, i, want, to, fly, from, boston, at, 838,...   \n",
       "1  [BOS, what, flights, are, available, from, pit...   \n",
       "2  [BOS, what, is, the, arrival, time, in, san, f...   \n",
       "3  [BOS, cheapest, airfare, from, tacoma, to, orl...   \n",
       "4  [BOS, round, trip, fares, from, pittsburgh, to...   \n",
       "\n",
       "                                               slots  \\\n",
       "0  [(BOS, O), (i, O), (want, O), (to, O), (fly, O...   \n",
       "1  [(BOS, O), (what, O), (flights, O), (are, O), ...   \n",
       "2  [(BOS, O), (what, O), (is, O), (the, O), (arri...   \n",
       "3  [(BOS, O), (cheapest, B-cost_relative), (airfa...   \n",
       "4  [(BOS, O), (round, B-round_trip), (trip, I-rou...   \n",
       "\n",
       "                                              vector  \n",
       "0  [-0.0010358343, 0.017182918, -0.041435994, 0.0...  \n",
       "1  [-0.0010358343, 0.017182918, -0.041435994, 0.0...  \n",
       "2  [-0.0010358343, 0.017182918, -0.041435994, 0.0...  \n",
       "3  [-0.0010358343, 0.017182918, -0.041435994, 0.0...  \n",
       "4  [-0.0010358343, 0.017182918, -0.041435994, 0.0...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:45:48.997799Z",
     "start_time": "2018-12-02T19:44:57.072551Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_shape = (len(train_df),nlp.vocab.vectors_length)\n",
    "X_train = np.zeros(X_train_shape)\n",
    "i =0\n",
    "for sentence in train_df['question'].values:\n",
    "    X_train[i,:] = nlp(sentence).vector\n",
    "    i +=1\n",
    "y_train = train_df['intent_val'].values\n",
    "\n",
    "X_test_shape = (len(test_df),nlp.vocab.vectors_length)\n",
    "X_test = np.zeros(X_test_shape)\n",
    "i =0\n",
    "for sentence in test_df['question'].values:\n",
    "    X_test[i,:] = nlp(sentence).vector\n",
    "    i +=1\n",
    "y_test= test_df['intent_val'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:45:51.336444Z",
     "start_time": "2018-12-02T19:45:51.326898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 5.16234040e-02,  2.32205659e-01, -3.59605029e-02, -2.82780938e-02,\n",
       "         2.63509810e-01,  3.42081189e-02, -3.79210524e-02, -2.16884981e-03,\n",
       "         1.39851958e-01,  1.66678202e+00, -2.65889585e-01, -1.23920456e-01,\n",
       "         8.70050937e-02, -7.50032812e-02, -1.43252000e-01,  2.13570893e-02,\n",
       "        -1.14725254e-01,  1.22758055e+00, -8.22240394e-03,  2.58636475e-02,\n",
       "         7.57709146e-02,  2.04386469e-02,  1.30797978e-02, -8.30395967e-02,\n",
       "        -5.63978478e-02,  4.81858626e-02, -2.49703020e-01, -1.11223243e-01,\n",
       "         9.84954089e-02,  1.17382910e-02, -6.74643889e-02,  6.22969046e-02,\n",
       "         2.26359032e-02,  2.95755357e-01, -7.27919042e-02, -9.55374688e-02,\n",
       "        -3.22525501e-02,  8.17344189e-02, -7.67499348e-03,  1.68229286e-02,\n",
       "        -1.44551426e-01,  5.18489480e-02, -1.71957344e-01,  6.66451678e-02,\n",
       "         1.13357425e-01,  1.89913243e-01, -7.01958537e-02, -5.28154485e-02,\n",
       "         1.23662297e-02, -4.70820963e-02, -1.54851954e-02,  7.63327032e-02,\n",
       "        -6.77193478e-02,  1.91443786e-02,  6.53008223e-02, -1.00382473e-02,\n",
       "        -6.98260367e-02, -3.63962501e-02, -7.19886273e-02, -2.04345975e-02,\n",
       "        -1.28577352e-01, -1.65410891e-01, -1.47118822e-01,  1.60548449e-01,\n",
       "        -2.50485949e-02, -1.31553262e-01,  2.48909481e-02,  1.41281724e-01,\n",
       "        -2.49755122e-02,  8.91093388e-02, -1.20617822e-01,  3.30094062e-02,\n",
       "         7.49693736e-02, -5.04147187e-02,  2.76356757e-01,  5.21708950e-02,\n",
       "         5.98296002e-02,  6.66157976e-02, -1.00780770e-01,  1.83833390e-01,\n",
       "         1.30954534e-02,  1.94546834e-01, -4.77145128e-02, -3.85196209e-02,\n",
       "         8.88057500e-02, -1.59184691e-02,  2.84562826e-01, -1.08035088e-01,\n",
       "         2.07361743e-01, -5.03762476e-02, -1.82192966e-01,  2.13520546e-02,\n",
       "        -5.74552491e-02, -1.79386765e-01,  1.41109213e-01, -1.66154583e-03,\n",
       "         3.34322564e-02,  1.58718526e-02, -7.71124065e-02,  5.34488559e-02,\n",
       "        -1.16407005e-02, -5.03139850e-03,  1.77614000e-02,  1.43502448e-02,\n",
       "         7.10345060e-02, -3.47466558e-01,  1.51344106e-01,  3.75915505e-02,\n",
       "         1.14251925e-02, -5.48549406e-02,  1.10185727e-01, -3.20895374e-01,\n",
       "         2.06274182e-01,  2.03119460e-02, -1.68631263e-02, -5.61008528e-02,\n",
       "         3.62628512e-02, -3.24735492e-02,  5.22242021e-03, -2.37329565e-02,\n",
       "         7.33646080e-02,  5.42811044e-02, -1.70907471e-02, -1.79679424e-01,\n",
       "         1.03316307e-01, -6.01476699e-04,  1.05780212e-03,  5.80594242e-02,\n",
       "         3.97039391e-03, -2.33464991e-03,  9.13376510e-02, -8.46858472e-02,\n",
       "        -2.27306396e-01,  2.52417438e-02,  1.04490146e-01, -1.79358453e-01,\n",
       "        -1.05144165e-01,  1.32967293e-01, -4.60328460e-02, -3.44389817e-03,\n",
       "        -1.02142107e+00, -7.76088461e-02,  9.87839922e-02, -1.09899543e-01,\n",
       "         1.43169999e-01,  1.34586394e-01, -6.80441037e-02, -2.13255025e-02,\n",
       "         1.84540022e-02, -9.97190475e-02,  6.06261007e-02, -7.31007531e-02,\n",
       "        -6.73687980e-02,  1.00620631e-02, -5.57521991e-02, -2.28114128e-02,\n",
       "         3.61204632e-02,  1.95215531e-02, -3.41949798e-02, -2.58981492e-02,\n",
       "         9.02873427e-02, -2.73430342e-04, -1.09228510e-02, -4.70854938e-02,\n",
       "         1.77299038e-01, -2.09847540e-02, -2.94534508e-02,  7.08173066e-02,\n",
       "         7.98075348e-02,  1.02906242e-01, -6.30559912e-03,  1.22639937e-02,\n",
       "        -8.57200027e-02, -9.61403549e-02, -2.40500574e-03,  1.25774950e-01,\n",
       "        -9.22490507e-02,  1.41818076e-02,  7.37087503e-02, -2.38016490e-02,\n",
       "        -1.10269189e-01, -1.43786788e-01, -2.16032509e-02, -5.15417531e-02,\n",
       "         1.27782509e-01,  7.32984543e-02, -2.37404685e-02, -3.82400975e-02,\n",
       "         2.56496787e-01,  5.47512472e-02, -6.23102002e-02,  1.32413819e-01,\n",
       "        -1.02975391e-01, -1.13672793e-01, -7.53530487e-02,  2.50484914e-01,\n",
       "        -4.28824499e-02, -2.37692036e-02, -1.02996305e-01, -4.83684540e-02,\n",
       "         1.50058553e-01, -1.00636102e-01, -4.80995998e-02,  6.60838559e-02,\n",
       "         2.35408813e-01, -2.97018997e-02,  3.97519805e-02,  1.17519675e-02,\n",
       "         4.48717549e-02, -4.61129025e-02,  3.03252526e-02,  6.06082492e-02,\n",
       "        -2.94358470e-02, -7.82102197e-02, -6.07210025e-02,  1.45875607e-02,\n",
       "        -7.97315985e-02,  5.30341081e-02, -6.69063106e-02,  4.55283150e-02,\n",
       "         8.29570442e-02, -1.31479293e-01, -7.48267919e-02, -1.28526315e-01,\n",
       "        -1.80173457e-01,  1.29172295e-01, -1.34822922e-02,  1.38439417e-01,\n",
       "         6.87581152e-02,  1.05624832e-01, -4.22634594e-02,  4.74815350e-03,\n",
       "         2.07389161e-01,  1.66124497e-02, -2.08233789e-01, -9.61079076e-02,\n",
       "        -2.25960582e-01, -1.39444143e-01, -4.88039590e-02,  1.72588393e-01,\n",
       "         1.55784845e-01,  1.65502094e-02,  9.48227718e-02, -2.17604544e-02,\n",
       "         6.11478910e-02, -5.76232001e-02, -7.84545541e-02,  6.32775575e-02,\n",
       "        -1.22951999e-01,  4.10535149e-02, -8.08020830e-02, -1.30863816e-01,\n",
       "        -6.34610131e-02, -6.06100038e-02,  2.70534426e-01,  1.50497660e-01,\n",
       "         1.08501807e-01,  1.79745965e-02, -1.46843702e-01, -1.14835061e-01,\n",
       "         1.92308038e-01,  7.38885030e-02,  1.00509822e-01,  4.74373698e-02,\n",
       "         1.69255529e-02, -3.40181887e-01, -1.32049158e-01,  1.48983151e-01,\n",
       "        -1.83778495e-01,  4.10862565e-02, -4.13127355e-02,  1.59160998e-02,\n",
       "        -5.32494672e-02,  7.44597837e-02,  1.33886486e-02,  5.00133447e-02,\n",
       "         9.36964080e-02,  3.68234441e-02, -2.47894712e-02,  1.87670395e-01,\n",
       "         7.20545426e-02, -6.57260567e-02, -7.56763443e-02, -6.60885219e-03,\n",
       "         6.24439046e-02,  2.57509705e-02,  1.09786011e-01,  8.87539983e-03,\n",
       "         7.18136057e-02,  1.59311205e-01, -1.43834949e-01,  9.11480039e-02,\n",
       "         5.78981042e-02, -1.06520511e-01, -2.13224236e-02, -7.40924254e-02,\n",
       "        -1.29365444e-01,  5.95930517e-02, -1.64653748e-01,  2.11117789e-01]),\n",
       " 14,\n",
       " array([ 2.22357288e-02,  1.32692039e-01, -1.09993331e-01, -1.30686507e-01,\n",
       "         3.41508150e-01, -1.74764991e-02,  9.70018134e-02, -5.15369065e-02,\n",
       "         1.38408646e-01,  1.73418629e+00, -4.41427112e-01, -1.09284401e-01,\n",
       "         1.12743348e-01, -3.69409472e-02, -1.69527993e-01, -1.21992089e-01,\n",
       "        -1.15268342e-01,  1.26884949e+00, -4.56656367e-02,  5.60768880e-03,\n",
       "         1.09908693e-01, -2.45031305e-02, -8.08681641e-03, -1.47680923e-01,\n",
       "        -8.92419592e-02,  1.03164546e-01, -2.32623056e-01, -2.05043972e-01,\n",
       "         1.59695938e-01,  1.70587946e-03,  1.68184061e-02, -1.43692316e-02,\n",
       "         5.65228686e-02,  2.50926703e-01,  6.99671805e-02, -1.13128893e-01,\n",
       "         5.68012968e-02, -5.67085519e-02, -1.32504165e-01, -1.13943689e-01,\n",
       "        -1.81499142e-02,  1.49572000e-01, -6.75995927e-03, -7.54327029e-02,\n",
       "         6.13673180e-02,  1.23375662e-01, -1.35057777e-01, -2.65444983e-02,\n",
       "        -2.41644820e-03,  4.17684503e-02, -1.29701182e-01,  2.29922459e-02,\n",
       "         2.28783190e-02,  1.50665278e-02,  8.40142220e-02, -4.74710902e-03,\n",
       "        -1.08002909e-02,  4.30576652e-02,  7.23294616e-02, -1.26511529e-01,\n",
       "        -1.21157169e-01, -1.07390836e-01, -7.86234513e-02,  8.44810084e-02,\n",
       "         5.57599068e-02, -4.97884974e-02, -2.01614061e-03,  8.22734609e-02,\n",
       "         2.57085972e-02,  5.35858748e-03, -2.49521248e-02,  5.60663007e-02,\n",
       "         8.74652565e-02, -2.16068644e-02,  1.80335820e-01,  1.45655926e-02,\n",
       "         6.89049670e-03,  4.08001840e-02, -3.28839272e-02,  2.38924071e-01,\n",
       "        -3.42370905e-02,  1.55849859e-01, -1.23364978e-01, -1.09791430e-02,\n",
       "         1.07572426e-03, -5.21531589e-02,  3.69973183e-01, -6.42923340e-02,\n",
       "         1.96287066e-01, -1.09685585e-01, -2.25847960e-01, -4.77508567e-02,\n",
       "        -7.62803853e-02, -2.22610369e-01,  4.15770561e-02,  1.32981956e-01,\n",
       "         6.98555931e-02, -3.13827321e-02, -6.28560036e-02, -2.92818388e-03,\n",
       "        -4.17483188e-02,  9.68084857e-02, -4.68394980e-02,  3.52791511e-02,\n",
       "         4.54522669e-03, -6.52641833e-01,  1.51287496e-01,  2.81811487e-02,\n",
       "        -1.07490547e-01,  6.33091526e-03,  6.10983409e-02, -1.59481466e-01,\n",
       "         1.11654736e-01, -7.90724829e-02, -7.28080571e-02, -2.43551214e-03,\n",
       "         9.32578295e-02, -6.99311346e-02,  1.95431174e-03, -7.21600056e-02,\n",
       "         2.94166040e-02,  3.57163884e-02,  3.90656851e-02, -9.42833498e-02,\n",
       "        -2.40354761e-02,  4.98792864e-02, -7.59018138e-02,  1.41492933e-02,\n",
       "         1.88917339e-01,  8.76652747e-02, -8.05323720e-02, -9.70702469e-02,\n",
       "        -1.60771906e-01, -3.66657749e-02,  9.84458402e-02, -8.01096782e-02,\n",
       "        -1.47287741e-01, -1.12652751e-02, -1.92288626e-02, -1.37680382e-01,\n",
       "        -1.39832926e+00,  4.61695530e-02,  1.32150888e-01, -8.03496912e-02,\n",
       "         5.40053137e-02, -1.78442281e-02, -3.19455042e-02, -1.78723652e-02,\n",
       "         1.94754917e-02, -1.55367941e-01,  3.79246809e-02, -4.97764573e-02,\n",
       "        -9.05664936e-02, -1.24500178e-01,  6.38032099e-03, -8.22187290e-02,\n",
       "         7.10828602e-03, -1.05626350e-02,  5.46748005e-02, -5.63392378e-02,\n",
       "         1.23320721e-01,  4.63124029e-02,  2.31279135e-02,  3.64829078e-02,\n",
       "         1.94475517e-01, -4.76906858e-02,  1.10193193e-02,  8.18889439e-02,\n",
       "         1.42477885e-01,  9.45954099e-02, -5.48287891e-02,  5.37369028e-02,\n",
       "        -8.90350249e-03, -2.81236530e-03,  5.44384681e-03,  6.14078157e-02,\n",
       "         7.52841262e-03,  4.38335501e-02,  4.21456546e-02,  3.52733657e-02,\n",
       "        -5.13665639e-02, -5.95003366e-04, -9.52798426e-02, -1.72759637e-01,\n",
       "         1.16240978e-01,  9.99341905e-02, -4.72666845e-02, -4.86504100e-02,\n",
       "         3.96952294e-02,  1.03821725e-01, -6.02874532e-02, -7.25908130e-02,\n",
       "        -2.05260534e-02, -3.50787789e-02,  4.28336859e-03,  1.50877401e-01,\n",
       "        -6.71605542e-02, -1.52558148e-01,  1.61685944e-02,  1.00969762e-01,\n",
       "         1.20723911e-01, -2.53744900e-01,  4.70184459e-04, -4.58221883e-02,\n",
       "         9.12190974e-02, -2.99126878e-02,  9.69465524e-02,  1.04143312e-02,\n",
       "         1.35639785e-02,  5.32224216e-03,  6.15240773e-03,  7.84568302e-03,\n",
       "        -5.46586290e-02, -8.80230367e-02, -4.08839583e-02,  8.09084103e-02,\n",
       "        -4.20935489e-02,  1.37789190e-01, -3.31484526e-02, -6.17403584e-03,\n",
       "         5.81326820e-02, -6.99164420e-02, -1.02471620e-01, -3.01444102e-02,\n",
       "        -6.91870973e-02,  6.17579930e-02, -1.58643350e-01,  2.13078007e-01,\n",
       "        -3.96771803e-02,  5.26750125e-02, -3.73376049e-02,  1.00812979e-01,\n",
       "         8.68648589e-02, -1.00526577e-02, -2.43275061e-01, -8.18903148e-02,\n",
       "        -3.28003205e-02, -1.04767770e-01, -5.07758111e-02,  2.32637540e-01,\n",
       "         2.21016303e-01,  3.38333994e-02,  9.62800309e-02, -3.72949094e-02,\n",
       "         1.24169327e-01, -1.13104194e-01,  7.31244758e-02, -1.41312294e-02,\n",
       "        -1.33784518e-01, -1.96880475e-02,  1.23245589e-01, -8.92747715e-02,\n",
       "         1.83464214e-03,  1.24155097e-02,  1.90066472e-01,  1.35706320e-01,\n",
       "         9.91417244e-02, -3.56882364e-02,  1.72585864e-02, -5.59995463e-03,\n",
       "         5.72885834e-02,  1.49519563e-01,  6.20982088e-02,  5.90588292e-03,\n",
       "         1.00998327e-01, -4.13085938e-01,  1.99859240e-03,  9.51984450e-02,\n",
       "        -6.67117676e-03,  1.10743038e-01, -1.62213311e-01,  7.37904087e-02,\n",
       "        -1.36533761e-02,  7.40173757e-02,  3.41319479e-02,  3.00046802e-02,\n",
       "        -5.78669943e-02, -6.84047788e-02,  1.23503737e-01,  1.08744547e-01,\n",
       "         4.08436637e-03, -6.14937320e-02,  2.50729676e-02,  3.86222266e-02,\n",
       "         2.21369080e-02,  1.34042721e-05,  1.31938949e-01, -7.15867728e-02,\n",
       "         1.90182492e-01,  1.16065061e-02,  1.00292750e-02,  1.29961908e-01,\n",
       "        -7.61712715e-02, -4.30745073e-02,  1.35261431e-01, -2.33758390e-02,\n",
       "        -9.52178538e-02, -8.89750384e-03,  5.07700443e-03,  2.22507775e-01]),\n",
       " 14)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0],y_train[0],X_test[0],y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:47:17.239675Z",
     "start_time": "2018-12-02T19:47:09.898298Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr1/anaconda3/envs/jupyter_env/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 684 correctly out of 893 test examples\n"
     ]
    }
   ],
   "source": [
    "# Import SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create a support vector classifier\n",
    "clf = SVC()\n",
    "\n",
    "# Fit the classifier using the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Count the number of correct predictions\n",
    "n_correct = 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_pred[i] == y_test[i]:\n",
    "        n_correct += 1\n",
    "\n",
    "print(\"Predicted {0} correctly out of {1} test examples\".format(n_correct, len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using spaCy's entity recogniser\n",
    "In this exercise you'll use spaCy's built-in entity recognizer to extract names, dates, and organizations from search queries. The spaCy library has been imported for you, and it's English model has been loaded as nlp.\n",
    "\n",
    "Your job is to define a function called extract_entities() which takes in a single argument message and returns a dictionary with the included entity types as keys, and the extracted entities as values. The included entity types are contained in a list called include_entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:47:25.335572Z",
     "start_time": "2018-12-02T19:47:25.299355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DATE': '2010', 'ORG': 'Google', 'PERSON': 'Mary'}\n",
      "{'DATE': '1999', 'ORG': 'MIT', 'PERSON': None}\n"
     ]
    }
   ],
   "source": [
    "# Define included entities\n",
    "include_entities = ['DATE', 'ORG', 'PERSON']\n",
    "\n",
    "# Define extract_entities()\n",
    "def extract_entities(message):\n",
    "    # Create a dict to hold the entities\n",
    "    ents = dict.fromkeys(include_entities)\n",
    "    # Create a spacy document\n",
    "    doc = nlp(message)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in include_entities:\n",
    "            # Save interesting entities\n",
    "            ents[ent.label_] = ent.text\n",
    "    return ents\n",
    "\n",
    "print(extract_entities('friends called Mary who have worked at Google since 2010'))\n",
    "print(extract_entities('people who graduated from MIT in 1999'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assigning roles using spaCy's parser\n",
    "In this exercise you'll use spaCy's powerful syntax parser to assign roles to the entities in your users' messages. To do this, you'll define two functions, find_parent_item() and assign_colors(). In doing so, you'll use a parse tree to assign roles, similar to how Alan did in the video.\n",
    "\n",
    "Recall that you can access the ancestors of a word using its .ancestors attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:47:33.299499Z",
     "start_time": "2018-12-02T19:47:33.279258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item: jacket has color : red\n",
      "item: jeans has color : blue\n"
     ]
    }
   ],
   "source": [
    "# Create the document\n",
    "doc = nlp(\"let's see that jacket in red and some blue jeans\")\n",
    "\n",
    "colors = ['black', 'red', 'blue']\n",
    "items = ['shoes', 'handback', 'jacket', 'jeans']\n",
    "def entity_type(word):\n",
    "    _type = None\n",
    "    if word.text in colors:\n",
    "        _type = \"color\"\n",
    "    elif word.text in items:\n",
    "        _type = \"item\"  \n",
    "    return _type\n",
    "\n",
    "# Iterate over parents in parse tree until an item entity is found\n",
    "def find_parent_item(word):\n",
    "    # Iterate over the word's ancestors\n",
    "    for parent in word.ancestors:\n",
    "        # Check for an \"item\" entity\n",
    "        if entity_type(parent) == \"item\":\n",
    "            return parent.text\n",
    "    return None\n",
    "\n",
    "# For all color entities, find their parent item\n",
    "def assign_colors(doc):\n",
    "    # Iterate over the document\n",
    "    for word in doc:\n",
    "        # Check for \"color\" entities\n",
    "        if entity_type(word) == \"color\":\n",
    "            # Find the parent\n",
    "            item =  find_parent_item(word)\n",
    "            print(\"item: {0} has color : {1}\".format(item, word))\n",
    "\n",
    "# Assign the colors\n",
    "assign_colors(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rasa NLU\n",
    "In this exercise you'll use Rasa NLU to create an interpreter, which parses incoming user messages and returns a set of entities. Your job is to train an interpreter using the MITIE entity recognition model in rasa NLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:47:41.801380Z",
     "start_time": "2018-12-02T19:47:41.709033Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "# Make sure you put the mitielib folder into the python search path.  There are\n",
    "# a lot of ways to do this, here we do it programmatically with the following\n",
    "# two statements:\n",
    "import os,sys\n",
    "parent = os.path.dirname(os.path.realpath('__file__'))\n",
    "url='/usr1/datascience/DataCampCourse/4_Building Chatbots in Python/supporting/MITIE-master/mitielib/'\n",
    "sys.path.append(url)\n",
    "from mitie import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:56:24.827954Z",
     "start_time": "2018-12-02T19:56:12.162181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "{'intent': {'name': '', 'confidence': 0.0}, 'entities': [], 'text': \"I'm looking for a Mexican restaurant in the North of town\"}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from rasa_nlu.converters import load_data\n",
    "#from rasa_nlu.training_data import load_data\n",
    "\n",
    "from rasa_nlu.config import RasaNLUConfig\n",
    "from rasa_nlu.model import Trainer\n",
    "\n",
    "# Create args dictionary\n",
    "args = {\"pipeline\":\"spacy_sklearn\"} # these are templates of pipelines \n",
    "#https://rasa.com/docs/nlu/0.11.4/pipeline/?highlight=spacy_sklearn\n",
    "args = {\"pipeline\":\"mitie\"}\n",
    "\n",
    "pipeline = [\"nlp_spacy\", \"tokenizer_spacy\", \"intent_entity_featurizer_regex\", \n",
    " \"intent_featurizer_spacy\", \"ner_crf\", \"ner_synonyms\",  \"intent_classifier_sklearn\"]\n",
    "\n",
    "pipeline = [\"intent_classifier_sklearn\"] # \"nlp_spacy\",\"ner_synonyms\"\n",
    "\n",
    "pipeline = [\"nlp_spacy\",\"ner_synonyms\",\"tokenizer_spacy\",\"intent_entity_featurizer_regex\",\"nlp_spacy\",\n",
    "           \"intent_featurizer_spacy\"]\n",
    "\n",
    "# Create a configuration and trainer\n",
    "#config = RasaNLUConfig(cmdline_args=args)\n",
    "config = RasaNLUConfig(cmdline_args={\"pipeline\": pipeline})\n",
    "trainer = Trainer(config)\n",
    "\n",
    "# Load the training data\n",
    "training_data = load_data(\"./supporting/rasa_nlu-master/datacamp/training_data.json\")\n",
    "\n",
    "# Create an interpreter by training the model\n",
    "interpreter = trainer.train(training_data)\n",
    "print(interpreter.parse(\"I'm looking for a Mexican restaurant in the North of town\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:56:27.711180Z",
     "start_time": "2018-12-02T19:56:27.685808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intent': {'name': '', 'confidence': 0.0}, 'entities': [], 'text': 'I m looking for a Mexican restaurant in the North of town'}\n"
     ]
    }
   ],
   "source": [
    "# Try it out\n",
    "#print(interpreter.parse(\"I am looking for a Mexican restaurant in the North of town\"))\n",
    "#print(interpreter.parse(\"I 'm looking for a Mexican restaurant in the North of town\"))\n",
    "print(interpreter.parse(\"I m looking for a Mexican restaurant in the North of town\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data-efficient entity recognition\n",
    "Most systems for extracting entities from text are built to extract 'Universal' things like names, dates, and places. But you probably don't have enough training data for your bot to make these systems perform well!\n",
    "\n",
    "In this exercise, you'll activate the MITIE entity recogniser inside rasa to extract restaurants-related entities using a very small amount of training data. A dictionary args has already been defined for you, along with a training_data object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T19:57:00.366312Z",
     "start_time": "2018-12-02T19:56:47.467604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intent': {'name': '', 'confidence': 0.0}, 'entities': [{'start': 28, 'end': 34, 'value': 'centre', 'entity': 'location', 'extractor': 'ner_crf'}], 'text': 'show me Chinese food in the centre of town'}\n",
      "{'intent': {'name': '', 'confidence': 0.0}, 'entities': [{'start': 10, 'end': 16, 'value': 'indian', 'entity': 'cuisine', 'extractor': 'ner_crf'}, {'start': 35, 'end': 39, 'value': 'west', 'entity': 'location', 'extractor': 'ner_crf'}], 'text': 'I want an Indian restaurant in the west'}\n",
      "{'intent': {'name': '', 'confidence': 0.0}, 'entities': [{'start': 39, 'end': 45, 'value': 'center', 'entity': 'location', 'extractor': 'ner_crf'}], 'text': 'are there any good pizza places in the center?'}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from rasa_nlu.config import RasaNLUConfig\n",
    "from rasa_nlu.model import Trainer\n",
    "\n",
    "pipeline = [\n",
    "    \"nlp_spacy\",\n",
    "    \"tokenizer_spacy\",\n",
    "    \"ner_crf\"\n",
    "]\n",
    "\n",
    "# Create a config that uses this pipeline\n",
    "config1 = RasaNLUConfig(cmdline_args={\"pipeline\": pipeline})\n",
    "\n",
    "# Create a trainer that uses this config\n",
    "trainer1 = Trainer(config1)\n",
    "\n",
    "# Create an interpreter by training the model\n",
    "interpreter1 = trainer1.train(training_data)\n",
    "\n",
    "# Parse some messages\n",
    "print(interpreter1.parse(\"show me Chinese food in the centre of town\"))\n",
    "print(interpreter1.parse(\"I want an Indian restaurant in the west\"))\n",
    "print(interpreter1.parse(\"are there any good pizza places in the center?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3 : Building a virtual assistant\n",
    "\n",
    "In this chapter, you're going to build a personal assistant to help you plan a trip. It will be able to respond to questions like \"are there any cheap hotels in the north of town?\" by looking inside a hotels database for matching results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T20:13:02.104660Z",
     "start_time": "2018-12-02T20:13:02.101603Z"
    }
   },
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtual Assistants and accessing data\n",
    "\n",
    "#### SQL basics\n",
    "Time to begin writing queries for your first hotel booking chatbot! The database has been loaded as \"hotels.db\", and a cursor which has access to the database has already been defined for you as cursor.\n",
    "\n",
    "Three queries are provided below. Your job is to identify which query returns ONLY the \"Hotel California\".\n",
    "\n",
    "You can test each query below by calling the cursor's .execute() method and passing the query in as a string. Then, you can print the results by calling the cursor's .fetchall() method, which takes no arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL statements in Python\n",
    "It's time to begin writing SQL queries! In this exercise, your job is to run a query against the hotels database to find all the expensive hotels in the south. The connection to the database has been created for you, along with a cursor c.\n",
    "\n",
    "As Alan described in the video, you should be careful about SQL injection. Here, you'll pass parameters the safe way: As an extra tuple argument to the .execute() method. This ensures malicious code can't be injected into your query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T18:08:43.630913Z",
     "start_time": "2018-11-15T18:08:43.627547Z"
    }
   },
   "source": [
    "### Exploring a DB with natural language\n",
    "\n",
    "#### Creating queries from parameters\n",
    "Now you're going to implement a more powerful function for querying the hotels database. The goal is to take arguments that can later be specified by other parts of your code.\n",
    "\n",
    "Specifically, your job here is to define a find_hotels() function which takes a single argument - a dictionary of column names and values - and returns a list of matching hotels from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T20:13:08.599920Z",
     "start_time": "2018-12-02T20:13:08.591020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM hotels WHERE area=? and price=? ('south', 'mid')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define find_hotels()\n",
    "def find_hotels(params):\n",
    "    # Create the base query\n",
    "    query = 'SELECT * FROM hotels'\n",
    "    # Add filter clauses for each of the parameters\n",
    "    if len(params) > 0:\n",
    "        filters = [\"{}=?\".format(k) for k in params]\n",
    "        query += \" WHERE \" + \" and \".join(filters)\n",
    "    # Create the tuple of values\n",
    "    #t = tuple([v for k,v in params.items()])\n",
    "    t = tuple(params.values())\n",
    "    print(query, t)\n",
    "    # Open connection to DB\n",
    "    conn = sqlite3.connect(\"./data/hotels.db\")\n",
    "    # Create a cursor\n",
    "    c = conn.cursor()\n",
    "    # Execute the query\n",
    "    c.execute(query, t)\n",
    "    # Return the results\n",
    "    return c.fetchall()\n",
    "    \n",
    "find_hotels({'area':'south','price':'mid'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using your custom function to find hotels\n",
    "Here, you're going to put your find_hotels() function into action! Recall that it accepts a single argument, params, which is a dictionary of column names and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T20:13:16.684958Z",
     "start_time": "2018-12-02T20:13:16.680560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM hotels WHERE area=? and price=? ('south', 'lo')\n",
      "[('Cozy Cottage', 'lo', 'south', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Create the dictionary of column names and values\n",
    "params = {'area':'south','price':'lo'}\n",
    "\n",
    "# Find the hotels that match the parameters\n",
    "print(find_hotels(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating SQL from natural language\n",
    "Now you'll write a respond() function which can handle messages like \"I want an expensive hotel in the south of town\" and respond appropriately according to the number of matching results in a database. This is important functionality for any database-backed chatbot.\n",
    "\n",
    "Your find_hotels() function from the previous exercises has already been defined for you, along with a rasa NLU interpreter object which can handle hotel queries and a list of responses, which you can explore in the Shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T20:17:55.672381Z",
     "start_time": "2018-12-02T20:17:55.637613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM hotels ()\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hotel for Dogs is one option, but I know others too :)'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses = [\"I'm sorry :( I couldn't find anything like that\",\n",
    " '{} is a great hotel!',\n",
    " '{} or {} would work!',\n",
    " '{} is one option, but I know others too :)']\n",
    "\n",
    "\n",
    "# Define respond()\n",
    "def respond(message):\n",
    "    # Extract the entities\n",
    "    entities = interpreter.parse(message)[\"entities\"]\n",
    "    # Initialize an empty params dictionary\n",
    "    params = {}\n",
    "    # Fill the dictionary with entities\n",
    "    for ent in entities:\n",
    "        params[ent[\"entity\"]] = str(ent[\"value\"])\n",
    "\n",
    "    # Find hotels that match the dictionary\n",
    "    results = find_hotels(params)\n",
    "    print(len(results))\n",
    "    # Get the names of the hotels and index of the response\n",
    "    names = [r[0] for r in results]\n",
    "    \n",
    "    n = min(len(results),3)\n",
    "    \n",
    "    # Select the nth element of the responses array\n",
    "    return responses[n].format(*names)\n",
    "    \n",
    "respond(\"I want an expensive hotel in the south of town\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental slot filling and negation\n",
    "\n",
    "#### Refining your search\n",
    "Now you'll write a bot that allows users to add filters incrementally, in case they don't specify all of their preferences in one message.\n",
    "\n",
    "To do this, initialize an empty dictionary params outside of your respond() function (unlike inside the function, like in the previous exercise). Your respond() function will take in this dictionary as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T20:18:03.982218Z",
     "start_time": "2018-12-02T20:18:03.938788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: I want an expensive hotel\n",
      "SELECT * FROM hotels ()\n",
      "BOT: Hotel for Dogs is one option, but I know others too :)\n",
      "USER: in the north of town\n",
      "SELECT * FROM hotels ()\n",
      "BOT: Hotel for Dogs is one option, but I know others too :)\n"
     ]
    }
   ],
   "source": [
    "# Define a respond function, taking the message and existing params as input\n",
    "def respond(message, params):\n",
    "    # Extract the entities\n",
    "    entities = interpreter.parse(message)[\"entities\"]\n",
    "    # Fill the dictionary with entities\n",
    "    for ent in entities:\n",
    "        params[ent[\"entity\"]] = str(ent[\"value\"])\n",
    "\n",
    "    # Find the hotels\n",
    "    results = find_hotels(params)\n",
    "    names = [r[0] for r in results]\n",
    "    n = min(len(results), 3)\n",
    "    # Return the appropriate response\n",
    "    return responses[n].format(*names), params\n",
    "\n",
    "# Initialize params dictionary\n",
    "params = {}\n",
    "\n",
    "# Pass the messages to the bot\n",
    "for message in [\"I want an expensive hotel\", \"in the north of town\"]:\n",
    "    print(\"USER: {}\".format(message))\n",
    "    response, params = respond(message, params)\n",
    "    print(\"BOT: {}\".format(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic negation\n",
    "Quite often you'll find your users telling you what they don't want - and that's important to understand! In general, negation is a difficult problem in NLP. Here we'll take a very simple approach that works for many cases.\n",
    "\n",
    "A list of tests called tests has been defined for you. Explore it in the Shell - you'll find that each test is a tuple consisting of:\n",
    "\n",
    "A string containing a message with entities\n",
    "A dictionary containing the entities as keys, and a Boolean saying whether they are negated as the key\n",
    "Your job is to define a function called negated_ents() which looks for negated entities in a message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T20:24:22.636038Z",
     "start_time": "2018-12-02T20:24:22.626826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "tests = [(\"no I don't want to be in the south\", {'south': False}),\n",
    " ('no it should be in the south', {'south': True}),\n",
    " ('no in the south not the north', {'north': False, 'south': True}),\n",
    " ('not north', {'north': False})]\n",
    "\n",
    "# Define negated_ents()\n",
    "def negated_ents(phrase,ents):\n",
    "    # Extract the entities using keyword matching\n",
    "    if len(ents) == 0:\n",
    "        ents = [e for e in [\"south\", \"north\"] if e in phrase]\n",
    "    # Find the index of the final character of each entity\n",
    "    ends = sorted([phrase.index(e) +len(e) for e in ents])\n",
    "    # Initialise a list to store sentence chunks\n",
    "    chunks = []\n",
    "    # Take slices of the sentence up to and including each entitiy\n",
    "    start = 0\n",
    "    for end in ends:\n",
    "        chunks.append(phrase[start:end])\n",
    "        start = end\n",
    "    result = {}\n",
    "    # Iterate over the chunks and look for entities\n",
    "    for chunk in chunks:\n",
    "        for ent in ents:\n",
    "            if ent in chunk:\n",
    "                # If the entity is preceeded by a negation, give it the key False\n",
    "                if \"not\" in chunk or \"n't\" in chunk:\n",
    "                    result[ent] = False\n",
    "                else:\n",
    "                    result[ent] = True\n",
    "    return result  \n",
    "\n",
    "# Check that the entities are correctly assigned as True or False\n",
    "for test in tests:\n",
    "    print(negated_ents(test[0],[]) == test[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering with excluded slots\n",
    "Now you're going to put together some of the ideas from previous exercises, and allow users to tell your bot about what they do and what they don't want, split across multiple messages.\n",
    "\n",
    "The negated_ents() function has already been defined for you. Additionally, a slightly tweaked version of the find_hotels() function, which accepts a neg_params dictionary in addition to a params dictionary, has been defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T20:31:06.168653Z",
     "start_time": "2018-12-02T20:31:06.119798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: I want a cheap hotel\n",
      "SELECT * FROM hotels ()\n",
      "BOT: Hotel for Dogs is one option, but I know others too :)\n",
      "USER: but not in the north of town\n",
      "SELECT * FROM hotels ()\n",
      "BOT: Hotel for Dogs is one option, but I know others too :)\n"
     ]
    }
   ],
   "source": [
    "# Define find_hotels()\n",
    "def find_hotels(params,neg_params):\n",
    "    # Create the base query\n",
    "    query = 'SELECT * FROM hotels'\n",
    "    # Add filter clauses for each of the parameters\n",
    "    if len(params) > 0:\n",
    "        filters = [\"{}=?\".format(k) for k in params] + [\"{}!=?\".format(k) for k in neg_params]\n",
    "        query += \" WHERE \" + \" and \".join(filters)\n",
    "    # Create the tuple of values\n",
    "    #t = tuple([v for k,v in params.items()])\n",
    "    t = tuple(params.values())\n",
    "    print(query, t)\n",
    "    # Open connection to DB\n",
    "    conn = sqlite3.connect(\"./data/hotels.db\")\n",
    "    # Create a cursor\n",
    "    c = conn.cursor()\n",
    "    # Execute the query\n",
    "    c.execute(query, t)\n",
    "    # Return the results\n",
    "    return c.fetchall()\n",
    "\n",
    "# Define the respond function\n",
    "def respond(message,params,neg_params):\n",
    "    # Extract the entities\n",
    "    entities = interpreter.parse(message)[\"entities\"]\n",
    "    ent_vals = [e[\"value\"] for e in entities]\n",
    "    # Look for negated entities\n",
    "    negated = negated_ents(message,ent_vals)\n",
    "    for ent in entities:\n",
    "        if ent[\"value\"] in negated and negated[ent[\"value\"]]:\n",
    "            neg_params[ent[\"entity\"]] = str(ent[\"value\"])\n",
    "        else:\n",
    "            params[ent[\"entity\"]] = str(ent[\"value\"])\n",
    "    # Find the hotels\n",
    "    results = find_hotels(params,neg_params)\n",
    "    names = [r[0] for r in results]\n",
    "    n = min(len(results),3)\n",
    "    # Return the correct response\n",
    "    return responses[n].format(*names), params, neg_params\n",
    "\n",
    "# Initialize params and neg_params\n",
    "params = {}\n",
    "neg_params = {}\n",
    "\n",
    "# Pass the messages to the bot\n",
    "for message in [\"I want a cheap hotel\", \"but not in the north of town\"]:\n",
    "    print(\"USER: {}\".format(message))\n",
    "    response, params, neg_params = respond(message, params, neg_params)\n",
    "    print(\"BOT: {}\".format(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4 : Dialogue\n",
    "\n",
    "Everything you've built so far has statelessly mapped intents to actions & responses. It's amazing how far you can get with that! But to build more sophisticated bots you will always want to add some statefulness. That's what you'll do here, as you build a chatbot that helps users order coffee. Have fun!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why statefulness is key\n",
    "\n",
    "#### Form filling\n",
    "You'll often want your bot to guide users through a series of steps, such as when they're placing an order.\n",
    "\n",
    "In this exercise, you'll begin building a bot that lets users order coffee. They can choose between two types: Colombian, and Kenyan. If the user provides unexpected input, your bot will handle this differently depending on where they are in the flow.\n",
    "\n",
    "Your job here is to identify the appropriate state and next state based on the intents and response messages provided. For example, if the intent is \"order\", then the state changes from INIT to CHOOSE_COFFEE.\n",
    "\n",
    "A function send_message(policy, state, message) has already been defined for you. It takes the policy, the current state and message as arguments, and returns the new state as a result. Additionally, an interpret(message) function, similar to the one Alan described in the video, has been pre-defined for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T20:59:17.404919Z",
     "start_time": "2018-12-02T20:59:17.394494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : I'd like to become a professional dancer\n",
      "BOT : I'm sorry - I'm not sure how to help you\n",
      "USER : well then I'd like to order some coffee\n",
      "BOT : ok, Columbian or Kenyan?\n",
      "USER : my favourite animal is a zebra\n",
      "BOT : I'm sorry - would you like Colombian or Kenyan?\n",
      "USER : kenyan\n",
      "BOT : perfect, the beans are on their way!\n"
     ]
    }
   ],
   "source": [
    "def send_message(policy, state, message):\n",
    "    print(\"USER : {}\".format(message))\n",
    "    new_state, response = respond(policy, state, message)\n",
    "    print(\"BOT : {}\".format(response))\n",
    "    return new_state\n",
    "\n",
    "def interpret(message):\n",
    "    msg = message.lower()\n",
    "    if 'order' in msg:\n",
    "        return 'order'\n",
    "    if 'kenyan' in msg or 'columbian' in msg:\n",
    "        return 'specify_coffee'\n",
    "    return 'none'\n",
    "\n",
    "def respond(policy, state, message): \n",
    "    (new_state, response) = policy[(state, interpret(message))] \n",
    "    return new_state, response\n",
    "\n",
    "\n",
    "# Define the INIT state\n",
    "INIT = 0\n",
    "\n",
    "# Define the CHOOSE_COFFEE state\n",
    "CHOOSE_COFFEE = 1\n",
    "\n",
    "# Define the ORDERED state\n",
    "ORDERED = 2\n",
    "\n",
    "# Define the policy rules\n",
    "policy = {\n",
    "    (INIT, \"order\"): (CHOOSE_COFFEE, \"ok, Columbian or Kenyan?\"),\n",
    "    (INIT, \"none\"): (INIT, \"I'm sorry - I'm not sure how to help you\"),\n",
    "    (CHOOSE_COFFEE, \"specify_coffee\"): (ORDERED, \"perfect, the beans are on their way!\"),\n",
    "    (CHOOSE_COFFEE, \"none\"): (CHOOSE_COFFEE, \"I'm sorry - would you like Colombian or Kenyan?\"),\n",
    "}\n",
    "\n",
    "# Create the list of messages\n",
    "messages = [\n",
    "    \"I'd like to become a professional dancer\",\n",
    "    \"well then I'd like to order some coffee\",\n",
    "    \"my favourite animal is a zebra\",\n",
    "    \"kenyan\"\n",
    "]\n",
    "\n",
    "# Call send_message() for each message\n",
    "state = INIT\n",
    "for message in messages:    \n",
    "    state = send_message(policy, state, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asking contextual questions\n",
    "Sometimes your users need some help! They will have questions and expect the bot to help them.\n",
    "\n",
    "In this exercise, you'll allow users to ask the coffee bot to explain the steps to them. Like before, the answer they get will depend on where they are in the flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T21:17:44.944364Z",
     "start_time": "2018-12-02T21:17:44.932438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : what can you do for me?\n",
      "0 what can you do for me?\n",
      "BOT : I'm a bot to help you order coffee beans\n",
      "0\n",
      "USER : well then I'd like to order some coffee\n",
      "0 well then I'd like to order some coffee\n",
      "BOT : ok, Columbian or Kenyan?\n",
      "1\n",
      "USER : what do you mean by that?\n",
      "1 what do you mean by that?\n",
      "BOT : ok, Columbian or Kenyan?\n",
      "1\n",
      "USER : kenyan\n",
      "1 kenyan\n",
      "BOT : perfect, the beans are on their way!\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Define the states\n",
    "INIT=0 \n",
    "CHOOSE_COFFEE=1\n",
    "ORDERED=2\n",
    "\n",
    "# Define the policy rules dictionary\n",
    "policy_rules = {\n",
    "    (INIT, \"ask_explanation\"): (INIT, \"I'm a bot to help you order coffee beans\"),\n",
    "    (INIT, \"none\"): (INIT, \"I'm a bot to help you order coffee beans\"),\n",
    "    (INIT, \"order\"): (CHOOSE_COFFEE, \"ok, Columbian or Kenyan?\"),\n",
    "    (CHOOSE_COFFEE, \"none\"): (CHOOSE_COFFEE, \"ok, Columbian or Kenyan?\"),\n",
    "    (CHOOSE_COFFEE, \"specify_coffee\"): (ORDERED, \"perfect, the beans are on their way!\"),\n",
    "    (CHOOSE_COFFEE, \"ask_explanation\"): (CHOOSE_COFFEE, \"We have two kinds of coffee beans - the Kenyan ones make a slightly sweeter coffee, and cost $6. The Brazilian beans make a nutty coffee and cost $5.\")    \n",
    "}\n",
    "\n",
    "def respond(state, message):\n",
    "    print(state, message)\n",
    "    (new_state, response) = policy_rules[(state, interpret(message))]\n",
    "    return new_state, response\n",
    "\n",
    "\n",
    "def send_message(state, message):\n",
    "    print(\"USER : {}\".format(message))\n",
    "    new_state, response = respond(state, message)\n",
    "    print(\"BOT : {}\".format(response))\n",
    "    return new_state\n",
    "\n",
    "# Define send_messages()\n",
    "def send_messages(messages):\n",
    "    state = INIT\n",
    "    for msg in messages:\n",
    "        state = send_message(state, msg)\n",
    "        print(state)\n",
    "\n",
    "# Send the messages\n",
    "send_messages([\n",
    "    \"what can you do for me?\",\n",
    "    \"well then I'd like to order some coffee\",\n",
    "    \"what do you mean by that?\",\n",
    "    \"kenyan\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with rejection\n",
    "What happens if you make a suggestion to your user, and they don't like it? Your bot will look really silly if it makes the same suggestion again right away.\n",
    "\n",
    "Here, you're going to modify your respond() function so that it accepts and returns 4 arguments:\n",
    "\n",
    "The user message as an argument, and the bot response as the first return value.\n",
    "A dictionary params including the entities the user has specified.\n",
    "A suggestions list. When passed to respond(), this should contain the suggestions made in the previous bot message. When returned by respond(), it should contain the current suggestions.\n",
    "An excluded list, which contains all of the results your user has already explicitly rejected.\n",
    "Your function should add the previous suggestions to the excluded list whenever it receives a \"deny\" intent. It should also filter out excluded suggestions from the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T22:01:39.042707Z",
     "start_time": "2018-12-02T22:01:38.994493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: I want a mid range hotel\n",
      "SELECT * FROM hotels ()\n",
      "BOT: Hotel for Dogs is one option, but I know others too :)\n",
      "********\n",
      "USER: no that doesn't work for me\n",
      "SELECT * FROM hotels ()\n",
      "BOT: Grand Hotel is one option, but I know others too :)\n",
      "********\n"
     ]
    }
   ],
   "source": [
    "def interpret(message):\n",
    "    data = interpreter.parse(message)\n",
    "    #print(data)\n",
    "    if 'no' in message:\n",
    "        data[\"intent\"][\"name\"] = \"deny\"\n",
    "    return data\n",
    "\n",
    "# Define respond()\n",
    "def respond(message,params,prev_suggestions,excluded):\n",
    "    # Interpret the message\n",
    "    parse_data = interpret(message)\n",
    "    # Extract the intent\n",
    "    intent = parse_data[\"intent\"][\"name\"]\n",
    "    # Extract the entities\n",
    "    entities = parse_data[\"entities\"]\n",
    "    # Add the suggestion to the excluded list if intent is \"deny\"\n",
    "    if intent == \"deny\":\n",
    "        excluded.extend(prev_suggestions)\n",
    "    # Fill the dictionary with entities\n",
    "    for ent in entities:\n",
    "        params[ent[\"entity\"]] = str(ent[\"value\"])\n",
    "    # Find matching hotels\n",
    "    results = [\n",
    "        r \n",
    "        for r in find_hotels(params, excluded) \n",
    "        if r[0] not in excluded\n",
    "    ]\n",
    "    # Extract the suggestions\n",
    "    names = [r[0] for r in results]\n",
    "    n = min(len(results), 3)\n",
    "    suggestions = names[:2]\n",
    "    return responses[n].format(*names), params, suggestions, excluded\n",
    "\n",
    "# Initialize the empty dictionary and lists\n",
    "params, suggestions, excluded = {}, [], []\n",
    "\n",
    "# Send the messages\n",
    "for message in [\"I want a mid range hotel\", \"no that doesn't work for me\"]:\n",
    "    print(\"USER: {}\".format(message))\n",
    "    response, params, suggestions, excluded = respond(message, params, suggestions, excluded)\n",
    "    print(\"BOT: {}\".format(response))\n",
    "    print(\"*\"*8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking questions & queuing answers\n",
    "\n",
    "#### Pending actions I\n",
    "You can really improve the user experience of your bot by asking them simple yes or no questions. One easy way to handle these follow-ups is to define pending actions which get executed as soon as the user says \"yes\", and wiped if the user says \"no\".\n",
    "\n",
    "In this exercise, you're going to define a policy function which takes the intent as it's sole argument, and returns two values: The next action to take, and a pending action. The policy function should return this value when a \"yes\" intent is returned, and should wipe the pending actions if a \"no\" intent is returned.\n",
    "\n",
    "Here, the interpret(message) function has been defined for you such that if \"yes\" is in the message, \"affirm\" is returned, and if \"no\" is in the message, then \"deny\" is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T22:16:19.370130Z",
     "start_time": "2018-12-02T22:16:19.364899Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define policy()\n",
    "def policy(intent):\n",
    "    # Return \"do_pending\" if the intent is \"affirm\"\n",
    "    if intent == \"affirm\":\n",
    "        return \"do_pending\", None\n",
    "    # Return \"Ok\" if the intent is \"deny\"\n",
    "    if intent == \"deny\":\n",
    "        return \"Ok\", None\n",
    "    if intent == \"order\":\n",
    "        return '''Unfortunately, the Kenyan coffee is currently out of stock, \n",
    "        would you like to order the Brazilian beans?''', \n",
    "        \"Alright, I've ordered that for you!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pending actions II\n",
    "Having defined your policy function, it's now time to write a send_message() function which takes both a pending action and a message as its arguments and leverages the policy function to determine the bot's response.\n",
    "\n",
    "Your policy(intent) function from the previous exercise has been pre-loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T22:23:54.023642Z",
     "start_time": "2018-12-02T22:23:54.016031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : I'd like to order some coffee\n",
      "BOT : Unfortunately, the Kenyan coffee is currently out of stock, would you like to order the Brazilian beans?\n",
      "USER : ok yes please\n",
      "BOT : Alright, I've ordered that for you!\n"
     ]
    }
   ],
   "source": [
    "def interpret(message):\n",
    "    msg = message.lower()\n",
    "    if 'order' in msg:\n",
    "        return 'order'\n",
    "    elif 'yes' in msg:\n",
    "        return 'affirm'\n",
    "    elif 'no' in msg:\n",
    "        return 'deny'\n",
    "    else:\n",
    "        return 'none'\n",
    "\n",
    "# Define send_message()\n",
    "def send_message(pending, message):\n",
    "    print(\"USER : {}\".format(message))\n",
    "    action, pending_action = policy(interpret(message))\n",
    "    if action == \"do_pending\" and pending is not None:\n",
    "        print(\"BOT : {}\".format(pending))\n",
    "    else:\n",
    "        print(\"BOT : {}\".format(action))\n",
    "    return pending_action\n",
    "    \n",
    "# Define send_messages()\n",
    "def send_messages(messages):\n",
    "    pending = None\n",
    "    for msg in messages:\n",
    "        pending = send_message(pending,msg)\n",
    "\n",
    "# Send the messages\n",
    "send_messages([\n",
    "    \"I'd like to order some coffee\",\n",
    "    \"ok yes please\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pending state transitions\n",
    "You'll often need to briefly deviate from a flow, for example to authenticate a user, before returning.\n",
    "\n",
    "In these cases, it's often simpler - and easier to debug - to save some actions/states as pending rather than adding ever more complicated rules.\n",
    "\n",
    "Here, you're going to define a policy_rules dictionary, where the keys are tuples of the current state and the received intent, while the values are tuples of the next state, the bot's response, and a state for which to set a pending transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T22:34:20.818811Z",
     "start_time": "2018-12-02T22:34:20.810465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : I'd like to order some coffee\n",
      "BOT : you'll have to log in first, what's your phone number?\n",
      "USER : 555-12345\n",
      "BOT : perfect, welcome back!\n",
      "BOT : would you like Columbian or Kenyan?\n",
      "USER : kenyan\n",
      "BOT : perfect, the beans are on their way!\n",
      "BOT : would you like Columbian or Kenyan?\n"
     ]
    }
   ],
   "source": [
    "# Define the states\n",
    "INIT=0\n",
    "AUTHED=1\n",
    "CHOOSE_COFFEE=2\n",
    "ORDERED=3\n",
    "\n",
    "# Define the policy rules\n",
    "policy_rules = {\n",
    "    (INIT, \"order\"): (AUTHED, \"you'll have to log in first, what's your phone number?\", AUTHED),\n",
    "    (AUTHED, \"number\"): (AUTHED, \"perfect, welcome back!\", None),\n",
    "    (AUTHED, \"none\"): (AUTHED, \"perfect, welcome back!\", None),\n",
    "    (AUTHED, \"order\"): (CHOOSE_COFFEE, \"would you like Columbian or Kenyan?\", None),    \n",
    "    (CHOOSE_COFFEE, \"specify_coffee\"): (ORDERED, \"perfect, the beans are on their way!\", None),\n",
    "    (CHOOSE_COFFEE, \"none\"): (ORDERED, \"perfect, the beans are on their way!\", None)\n",
    "}\n",
    "\n",
    "def send_message(state, pending, message):\n",
    "    print(\"USER : {}\".format(message))\n",
    "    new_state, response, pending_state = policy_rules[(state, interpret(message))]\n",
    "    print(\"BOT : {}\".format(response))\n",
    "    if pending is not None:\n",
    "        new_state, response, pending_state = policy_rules[pending]\n",
    "        print(\"BOT : {}\".format(response))\n",
    "    if pending_state is not None:\n",
    "        pending = (pending_state, interpret(message))\n",
    "    return new_state, pending\n",
    "        \n",
    "# Define send_messages()\n",
    "def send_messages(messages):\n",
    "    state = INIT\n",
    "    pending = None\n",
    "    for msg in messages:\n",
    "        state, pending = send_message(state, pending, msg)\n",
    "\n",
    "# Send the messages\n",
    "send_messages([\n",
    "    \"I'd like to order some coffee\",\n",
    "    \"555-12345\",\n",
    "    \"kenyan\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all together I\n",
    "It's time to put everything together everything you've learned in the course by combining the coffee ordering bot with the eliza rules from chapter 1.\n",
    "\n",
    "To begin, you'll define a function called chitchat_response(), which calls the predefined function match_rule() from back in chapter 1. This returns a response if the message matched an eliza template, and otherwise, None.\n",
    "\n",
    "The eliza rules are contained in a dictionary called eliza_rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T22:45:53.734093Z",
     "start_time": "2018-12-02T22:45:53.728824Z"
    }
   },
   "outputs": [],
   "source": [
    "eliza_rules = {'I want (.*)': ['What would it mean if you got {0}',\n",
    "  'Why do you want {0}',\n",
    "  \"What's stopping you from getting {0}\"],\n",
    " 'do you remember (.*)': ['Did you think I would forget {0}',\n",
    "  \"Why haven't you been able to forget {0}\",\n",
    "  'What about {0}',\n",
    "  'Yes .. and?'],\n",
    " 'do you think (.*)': ['if {0}? Absolutely.', 'No chance'],\n",
    " 'if (.*)': [\"Do you really think it's likely that {0}\",\n",
    "  'Do you wish that {0}',\n",
    "  'What do you think about {0}',\n",
    "  'Really--if {0}']}\n",
    "\n",
    "# Define chitchat_response()\n",
    "def chitchat_response(message):\n",
    "    # Call match_rule()\n",
    "    response, phrase = match_rule(eliza_rules,message)\n",
    "    # Return none is response is \"default\"\n",
    "    if response == \"default\":\n",
    "        return None\n",
    "    if '{0}' in response:\n",
    "        # Replace the pronouns of phrase\n",
    "        phrase = replace_pronouns(phrase)\n",
    "        # Calculate the response\n",
    "        response = response.format(phrase)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all together II\n",
    "With your chitchat_response(message) function defined, the next step is to define a send_message() function which first calls chitchat_response(message), and only uses the coffee bot policy if there is no matching message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T22:48:38.237628Z",
     "start_time": "2018-12-02T22:48:38.229271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : I'd like to order some coffee\n",
      "BOT : you'll have to log in first, what's your phone number?\n",
      "USER : 555-12345\n",
      "BOT : perfect, welcome back!\n",
      "BOT : would you like Columbian or Kenyan?\n",
      "USER : do you remember when I ordered 1000 kilos by accident?\n",
      "BOT : Yes .. and?\n",
      "USER : kenyan\n",
      "BOT : perfect, the beans are on their way!\n"
     ]
    }
   ],
   "source": [
    "# Define send_message()\n",
    "def send_message(state,pending,message):\n",
    "    print(\"USER : {}\".format(message))\n",
    "    response = chitchat_response(message)\n",
    "    if response is not None:\n",
    "        print(\"BOT : {}\".format(response))\n",
    "        return state, None\n",
    "    \n",
    "    # Calculate the new_state, response, and pending_state\n",
    "    new_state, response, pending_state = policy_rules[(state, interpret(message))]\n",
    "    print(\"BOT : {}\".format(response))\n",
    "    if pending is not None:\n",
    "        new_state, response, pending_state = policy_rules[pending]\n",
    "        print(\"BOT : {}\".format(response))        \n",
    "    if pending_state is not None:\n",
    "        pending = (pending_state, interpret(message))\n",
    "    return new_state, pending\n",
    "\n",
    "# Define send_messages()\n",
    "def send_messages(messages):\n",
    "    state = INIT\n",
    "    pending = None\n",
    "    for msg in messages:\n",
    "        state, pending = send_message(state, pending, msg)\n",
    "\n",
    "# Send the messages\n",
    "send_messages([\n",
    "    \"I'd like to order some coffee\",\n",
    "    \"555-12345\",\n",
    "    \"do you remember when I ordered 1000 kilos by accident?\",\n",
    "    \"kenyan\"\n",
    "])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frontiers of dialogue research\n",
    "\n",
    "#### Generating text with neural networks\n",
    "In this final exercise of the course, you're going to generate text using a neural network trained on the scripts of every episode of The Simpsons. Specifically, you'll use a simplified version of the sample_text() function that Alan described in the video.\n",
    "\n",
    "It takes in two arguments, seed, and temperature. The seed argument is the initial sequence that the network uses to generate the subsequent text, while the temperature argument controls how risky the network is when generating text. At very low temperatures, it just repeats the most common combinations of letters, and at very high temperatures it generates complete gibberish. In order to ensure fast runtimes, the network in this exercise will only work for the subset of temperature values.\n",
    "\n",
    "After you finish this exercise, be sure to check out this tutorial by Alan in which he walks you through how to connect a chatbot to Facebook Messenger!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T22:56:40.848669Z",
     "start_time": "2018-12-02T22:56:40.842711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating text with riskiness : 0.2\n",
      "\n",
      "i'm gonna punch lenny in the back of the been a to the on the man to the mother and the father to simpson the father to with the marge in the for the like the fame to the been to the for my bart the don't was in the like the for the father the father a was the father been a say the been to me the do it and the father been to go. i want to the boy i can the from a man to be the for the been a like the father to make my bart of the father\n",
      "\n",
      "Generating text with riskiness : 0.5\n",
      "\n",
      "i'm gonna punch lenny in the back of the kin't she change and i'm all better it and the was the fad a drivera it? what i want to did hey, he would you would in your bus who know is the like and this don't are for your this all for your manset the for it a man is on the see the will they want to know i'm are for one start of that and i got the better this is. it whoce and i don't are on the mater stop in the from a for the be your mileat\n",
      "\n",
      "Generating text with riskiness : 1.0\n",
      "\n",
      "i'm gonna punch lenny in the back of the to to macks how screath. firl done we wouldn't wil that kill. of this torshmobote since, i know i ord did, can give crika of sintenn prescoam.whover my me after may? there's right. that up. there's ruining isay.oh.solls.nan'h those off point chuncing car your anal medion.hey, are exallies a off while bea dolk of sure, hello, no in her, we'll rundems... i'm eventy taving me to too the letberngonce\n",
      "\n",
      "Generating text with riskiness : 1.2\n",
      "\n",
      "i'm gonna punch lenny in the back of the burear prespe-nakes, 'lisa to isn't that godios.and when be the bowniday' would lochs meine, mind crikvin' suhle ovotaci!..... hey, a poielyfd othe flancer, this in are rightplouten of of we doll hurrs, truelturone? rake inswaydan justy!we scrikent.ow.. by back hous, smadge, the lighel irely.yes, homer. wel'e esasmoy ryelalrs all wronencay...... nank. i wenth makedyk. come on help cerzind, now, n\n"
     ]
    }
   ],
   "source": [
    "generated = {0.2: \"i'm gonna punch lenny in the back of the been a to the on the man to the mother and the father to simpson the father to with the marge in the for the like the fame to the been to the for my bart the don't was in the like the for the father the father a was the father been a say the been to me the do it and the father been to go. i want to the boy i can the from a man to be the for the been a like the father to make my bart of the father\",\n",
    " 0.5: \"i'm gonna punch lenny in the back of the kin't she change and i'm all better it and the was the fad a drivera it? what i want to did hey, he would you would in your bus who know is the like and this don't are for your this all for your manset the for it a man is on the see the will they want to know i'm are for one start of that and i got the better this is. it whoce and i don't are on the mater stop in the from a for the be your mileat\",\n",
    " 1.0: \"i'm gonna punch lenny in the back of the to to macks how screath. firl done we wouldn't wil that kill. of this torshmobote since, i know i ord did, can give crika of sintenn prescoam.whover my me after may? there's right. that up. there's ruining isay.oh.solls.nan'h those off point chuncing car your anal medion.hey, are exallies a off while bea dolk of sure, hello, no in her, we'll rundems... i'm eventy taving me to too the letberngonce\",\n",
    " 1.2: \"i'm gonna punch lenny in the back of the burear prespe-nakes, 'lisa to isn't that godios.and when be the bowniday' would lochs meine, mind crikvin' suhle ovotaci!..... hey, a poielyfd othe flancer, this in are rightplouten of of we doll hurrs, truelturone? rake inswaydan justy!we scrikent.ow.. by back hous, smadge, the lighel irely.yes, homer. wel'e esasmoy ryelalrs all wronencay...... nank. i wenth makedyk. come on help cerzind, now, n\"}\n",
    "def sample_text(seed, temperature):\n",
    "    return generated[temperature]\n",
    "\n",
    "# Feed the 'seed' text into the neural network\n",
    "seed = \"i'm gonna punch lenny in the back of the\"\n",
    "\n",
    "# Iterate over the different temperature values\n",
    "for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "    print(\"\\nGenerating text with riskiness : {}\\n\".format(temperature))\n",
    "    # Call the sample_text function\n",
    "    print(sample_text(seed,temperature))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
